{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "edaadb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession. \\\n",
    "builder. \\\n",
    "appName(\"Sneha Spark Session\").\\\n",
    "config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "enableHiveSupport(). \\\n",
    "master('yarn'). \\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "71d7e118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://g01.itversity.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Sneha Spark Session</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4aec2284e0>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "15bf32a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_df = spark.read.format('csv').option(\"header\", \"True\").load('/public/trendytech/datasets/hospital.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "139c855c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- patient_id: string (nullable = true)\n",
      " |-- admission_date: string (nullable = true)\n",
      " |-- discharge_date: string (nullable = true)\n",
      " |-- diagnosis: string (nullable = true)\n",
      " |-- doctor_id: string (nullable = true)\n",
      " |-- total_cost: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hospital_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "08ec177e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "496692d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_new_df = hospital_df.withColumn(\"admission_date\",to_date(\"admission_date\",\"mm-dd-yyyy\")).withColumn(\"discharge_date\",to_date(\"discharge_date\",\"yyyy-mm-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "adac7e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- patient_id: string (nullable = true)\n",
      " |-- admission_date: date (nullable = true)\n",
      " |-- discharge_date: date (nullable = true)\n",
      " |-- diagnosis: string (nullable = true)\n",
      " |-- doctor_id: string (nullable = true)\n",
      " |-- total_cost: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hospital_new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "fc4770e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_new_df.createOrReplaceTempView(\"hospital\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "df598343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>count(1)</th></tr>\n",
       "<tr><td>25</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+\n",
       "|count(1)|\n",
       "+--------+\n",
       "|      25|\n",
       "+--------+"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from hospital\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a1d1f69f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>patient_id</th><th>admission_date</th><th>discharge_date</th><th>diagnosis</th><th>doctor_id</th><th>total_cost</th></tr>\n",
       "<tr><td>10</td><td>2022-01-05</td><td>2022-01-10</td><td>Appendicitis</td><td>110</td><td>7500.00</td></tr>\n",
       "<tr><td>2</td><td>2022-01-05</td><td>2022-01-09</td><td>Appendicitis</td><td>102</td><td>7000.00</td></tr>\n",
       "<tr><td>14</td><td>2023-01-14</td><td>2023-01-18</td><td>Appendicitis</td><td>114</td><td>7200.00</td></tr>\n",
       "<tr><td>6</td><td>2022-01-10</td><td>2022-01-15</td><td>Appendicitis</td><td>106</td><td>8000.00</td></tr>\n",
       "<tr><td>20</td><td>2023-01-10</td><td>2023-01-16</td><td>Appendicitis</td><td>120</td><td>7800.00</td></tr>\n",
       "<tr><td>3</td><td>2022-01-12</td><td>2022-01-18</td><td>Fractured Arm</td><td>103</td><td>3500.00</td></tr>\n",
       "<tr><td>15</td><td>2023-01-20</td><td>2023-01-28</td><td>Fractured Arm</td><td>115</td><td>3800.00</td></tr>\n",
       "<tr><td>24</td><td>2023-01-01</td><td>2023-01-07</td><td>Fractured Arm</td><td>124</td><td>4100.00</td></tr>\n",
       "<tr><td>9</td><td>2022-01-15</td><td>2022-01-22</td><td>Fractured Leg</td><td>109</td><td>6000.00</td></tr>\n",
       "<tr><td>19</td><td>2023-01-22</td><td>2023-01-27</td><td>Fractured Leg</td><td>119</td><td>6500.00</td></tr>\n",
       "<tr><td>8</td><td>2022-01-25</td><td>2022-01-01</td><td>Heart Attack</td><td>108</td><td>20000.00</td></tr>\n",
       "<tr><td>13</td><td>2023-01-02</td><td>2023-01-09</td><td>Heart Attack</td><td>113</td><td>18000.00</td></tr>\n",
       "<tr><td>17</td><td>2023-01-08</td><td>2023-01-11</td><td>Heart Attack</td><td>117</td><td>16000.00</td></tr>\n",
       "<tr><td>22</td><td>2023-01-12</td><td>2023-01-19</td><td>Heart Attack</td><td>122</td><td>21000.00</td></tr>\n",
       "<tr><td>4</td><td>2022-01-02</td><td>2022-01-08</td><td>Heart Attack</td><td>104</td><td>15000.00</td></tr>\n",
       "<tr><td>5</td><td>2022-01-05</td><td>2022-01-07</td><td>Influenza</td><td>105</td><td>2500.00</td></tr>\n",
       "<tr><td>16</td><td>2023-01-05</td><td>2023-01-11</td><td>Influenza</td><td>116</td><td>2700.00</td></tr>\n",
       "<tr><td>11</td><td>2022-01-02</td><td>2022-01-05</td><td>Influenza</td><td>111</td><td>2800.00</td></tr>\n",
       "<tr><td>21</td><td>2023-01-05</td><td>2023-01-09</td><td>Influenza</td><td>121</td><td>2900.00</td></tr>\n",
       "<tr><td>25</td><td>2024-01-10</td><td>2024-01-15</td><td>Influenza</td><td>125</td><td>3200.00</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+----------+--------------+--------------+-------------+---------+----------+\n",
       "|patient_id|admission_date|discharge_date|    diagnosis|doctor_id|total_cost|\n",
       "+----------+--------------+--------------+-------------+---------+----------+\n",
       "|        10|    2022-01-05|    2022-01-10| Appendicitis|      110|   7500.00|\n",
       "|         2|    2022-01-05|    2022-01-09| Appendicitis|      102|   7000.00|\n",
       "|        14|    2023-01-14|    2023-01-18| Appendicitis|      114|   7200.00|\n",
       "|         6|    2022-01-10|    2022-01-15| Appendicitis|      106|   8000.00|\n",
       "|        20|    2023-01-10|    2023-01-16| Appendicitis|      120|   7800.00|\n",
       "|         3|    2022-01-12|    2022-01-18|Fractured Arm|      103|   3500.00|\n",
       "|        15|    2023-01-20|    2023-01-28|Fractured Arm|      115|   3800.00|\n",
       "|        24|    2023-01-01|    2023-01-07|Fractured Arm|      124|   4100.00|\n",
       "|         9|    2022-01-15|    2022-01-22|Fractured Leg|      109|   6000.00|\n",
       "|        19|    2023-01-22|    2023-01-27|Fractured Leg|      119|   6500.00|\n",
       "|         8|    2022-01-25|    2022-01-01| Heart Attack|      108|  20000.00|\n",
       "|        13|    2023-01-02|    2023-01-09| Heart Attack|      113|  18000.00|\n",
       "|        17|    2023-01-08|    2023-01-11| Heart Attack|      117|  16000.00|\n",
       "|        22|    2023-01-12|    2023-01-19| Heart Attack|      122|  21000.00|\n",
       "|         4|    2022-01-02|    2022-01-08| Heart Attack|      104|  15000.00|\n",
       "|         5|    2022-01-05|    2022-01-07|    Influenza|      105|   2500.00|\n",
       "|        16|    2023-01-05|    2023-01-11|    Influenza|      116|   2700.00|\n",
       "|        11|    2022-01-02|    2022-01-05|    Influenza|      111|   2800.00|\n",
       "|        21|    2023-01-05|    2023-01-09|    Influenza|      121|   2900.00|\n",
       "|        25|    2024-01-10|    2024-01-15|    Influenza|      125|   3200.00|\n",
       "+----------+--------------+--------------+-------------+---------+----------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from hospital order by diagnosis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964bdbd1",
   "metadata": {},
   "source": [
    "### Running Total of the total_cost based on diagnosis order by admission_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0e849372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "98b59801",
   "metadata": {},
   "outputs": [],
   "source": [
    "mywindow = Window.partitionBy(\"diagnosis\").orderBy(\"admission_date\").rowsBetween(Window.unboundedPreceding,Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "9ec6385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_running = hospital_new_df.withColumn(\"running_total\",sum(\"total_cost\").over(mywindow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "9e7adcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+---------+----------+-------------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|doctor_id|total_cost|running_total|\n",
      "+----------+--------------+--------------+-------------+---------+----------+-------------+\n",
      "|         4|    2022-01-02|    2022-01-08| Heart Attack|      104|  15000.00|      15000.0|\n",
      "|         8|    2022-01-25|    2022-01-01| Heart Attack|      108|  20000.00|      35000.0|\n",
      "|        13|    2023-01-02|    2023-01-09| Heart Attack|      113|  18000.00|      53000.0|\n",
      "|        17|    2023-01-08|    2023-01-11| Heart Attack|      117|  16000.00|      69000.0|\n",
      "|        22|    2023-01-12|    2023-01-19| Heart Attack|      122|  21000.00|      90000.0|\n",
      "|         3|    2022-01-12|    2022-01-18|Fractured Arm|      103|   3500.00|       3500.0|\n",
      "|        24|    2023-01-01|    2023-01-07|Fractured Arm|      124|   4100.00|       7600.0|\n",
      "|        15|    2023-01-20|    2023-01-28|Fractured Arm|      115|   3800.00|      11400.0|\n",
      "|         9|    2022-01-15|    2022-01-22|Fractured Leg|      109|   6000.00|       6000.0|\n",
      "|        19|    2023-01-22|    2023-01-27|Fractured Leg|      119|   6500.00|      12500.0|\n",
      "|         2|    2022-01-05|    2022-01-09| Appendicitis|      102|   7000.00|       7000.0|\n",
      "|        10|    2022-01-05|    2022-01-10| Appendicitis|      110|   7500.00|      14500.0|\n",
      "|         6|    2022-01-10|    2022-01-15| Appendicitis|      106|   8000.00|      22500.0|\n",
      "|        20|    2023-01-10|    2023-01-16| Appendicitis|      120|   7800.00|      30300.0|\n",
      "|        14|    2023-01-14|    2023-01-18| Appendicitis|      114|   7200.00|      37500.0|\n",
      "|        11|    2022-01-02|    2022-01-05|    Influenza|      111|   2800.00|       2800.0|\n",
      "|         5|    2022-01-05|    2022-01-07|    Influenza|      105|   2500.00|       5300.0|\n",
      "|        16|    2023-01-05|    2023-01-11|    Influenza|      116|   2700.00|       8000.0|\n",
      "|        21|    2023-01-05|    2023-01-09|    Influenza|      121|   2900.00|      10900.0|\n",
      "|        25|    2024-01-10|    2024-01-15|    Influenza|      125|   3200.00|      14100.0|\n",
      "+----------+--------------+--------------+-------------+---------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_running.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6e942ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+---------+----------+-------------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|doctor_id|total_cost|running_total|\n",
      "+----------+--------------+--------------+-------------+---------+----------+-------------+\n",
      "|         4|    2022-01-02|    2022-01-08| Heart Attack|      104|  15000.00|      15000.0|\n",
      "|         8|    2022-01-25|    2022-01-01| Heart Attack|      108|  20000.00|      35000.0|\n",
      "|        13|    2023-01-02|    2023-01-09| Heart Attack|      113|  18000.00|      53000.0|\n",
      "|        17|    2023-01-08|    2023-01-11| Heart Attack|      117|  16000.00|      69000.0|\n",
      "|        22|    2023-01-12|    2023-01-19| Heart Attack|      122|  21000.00|      90000.0|\n",
      "|         3|    2022-01-12|    2022-01-18|Fractured Arm|      103|   3500.00|       3500.0|\n",
      "|        24|    2023-01-01|    2023-01-07|Fractured Arm|      124|   4100.00|       7600.0|\n",
      "|        15|    2023-01-20|    2023-01-28|Fractured Arm|      115|   3800.00|      11400.0|\n",
      "|         9|    2022-01-15|    2022-01-22|Fractured Leg|      109|   6000.00|       6000.0|\n",
      "|        19|    2023-01-22|    2023-01-27|Fractured Leg|      119|   6500.00|      12500.0|\n",
      "|         2|    2022-01-05|    2022-01-09| Appendicitis|      102|   7000.00|      14500.0|\n",
      "|        10|    2022-01-05|    2022-01-10| Appendicitis|      110|   7500.00|      14500.0|\n",
      "|         6|    2022-01-10|    2022-01-15| Appendicitis|      106|   8000.00|      22500.0|\n",
      "|        20|    2023-01-10|    2023-01-16| Appendicitis|      120|   7800.00|      30300.0|\n",
      "|        14|    2023-01-14|    2023-01-18| Appendicitis|      114|   7200.00|      37500.0|\n",
      "|        11|    2022-01-02|    2022-01-05|    Influenza|      111|   2800.00|       2800.0|\n",
      "|         5|    2022-01-05|    2022-01-07|    Influenza|      105|   2500.00|       5300.0|\n",
      "|        16|    2023-01-05|    2023-01-11|    Influenza|      116|   2700.00|      10900.0|\n",
      "|        21|    2023-01-05|    2023-01-09|    Influenza|      121|   2900.00|      10900.0|\n",
      "|        25|    2024-01-10|    2024-01-15|    Influenza|      125|   3200.00|      14100.0|\n",
      "+----------+--------------+--------------+-------------+---------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select patient_id,admission_date,discharge_date,diagnosis,doctor_id,total_cost, \n",
    "sum(total_cost) over (partition by diagnosis order by admission_date) as running_total from hospital\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "4cfb5e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### total cost of each diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "bf1f27cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|    diagnosis|totaldiagnosiscost|\n",
      "+-------------+------------------+\n",
      "| Heart Attack|           90000.0|\n",
      "|Fractured Arm|           11400.0|\n",
      "|Fractured Leg|           12500.0|\n",
      "| Appendicitis|           37500.0|\n",
      "|    Influenza|           14100.0|\n",
      "|    Pneumonia|           26500.0|\n",
      "+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_total_cost = hospital_new_df.groupBy(\"diagnosis\").agg(sum(\"total_cost\").alias(\"totaldiagnosiscost\"))\n",
    "result_total_cost.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "39ccf9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|    diagnosis|totaldiagnosiscost|\n",
      "+-------------+------------------+\n",
      "| Heart Attack|           90000.0|\n",
      "|Fractured Arm|           11400.0|\n",
      "|Fractured Leg|           12500.0|\n",
      "| Appendicitis|           37500.0|\n",
      "|    Influenza|           14100.0|\n",
      "|    Pneumonia|           26500.0|\n",
      "+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select diagnosis,sum(total_cost)as totaldiagnosiscost from hospital group by diagnosis\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "17fda3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2nd highest total_cost in each diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "8f9d9f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "343acb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mywindow = Window.partitionBy(\"diagnosis\").orderBy(desc(\"total_cost\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a2a184f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+---------+----------+---------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|doctor_id|total_cost|denserank|\n",
      "+----------+--------------+--------------+-------------+---------+----------+---------+\n",
      "|         8|    2022-01-25|    2022-01-01| Heart Attack|      108|  20000.00|        2|\n",
      "|        15|    2023-01-20|    2023-01-28|Fractured Arm|      115|   3800.00|        2|\n",
      "|         9|    2022-01-15|    2022-01-22|Fractured Leg|      109|   6000.00|        2|\n",
      "|        20|    2023-01-10|    2023-01-16| Appendicitis|      120|   7800.00|        2|\n",
      "|        21|    2023-01-05|    2023-01-09|    Influenza|      121|   2900.00|        2|\n",
      "|         7|    2022-01-20|    2022-01-25|    Pneumonia|      107|   5500.00|        2|\n",
      "+----------+--------------+--------------+-------------+---------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_total_cost = hospital_new_df.withColumn(\"denserank\",dense_rank().over(mywindow))\n",
    "result_total_cost.filter(\"denserank ==2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "1331ae63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+---------+----------+---------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|doctor_id|total_cost|denserank|\n",
      "+----------+--------------+--------------+-------------+---------+----------+---------+\n",
      "|         8|    2022-01-25|    2022-01-01| Heart Attack|      108|  20000.00|        2|\n",
      "|        15|    2023-01-20|    2023-01-28|Fractured Arm|      115|   3800.00|        2|\n",
      "|         9|    2022-01-15|    2022-01-22|Fractured Leg|      109|   6000.00|        2|\n",
      "|        20|    2023-01-10|    2023-01-16| Appendicitis|      120|   7800.00|        2|\n",
      "|        21|    2023-01-05|    2023-01-09|    Influenza|      121|   2900.00|        2|\n",
      "|         7|    2022-01-20|    2022-01-25|    Pneumonia|      107|   5500.00|        2|\n",
      "+----------+--------------+--------------+-------------+---------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" with cte as (select patient_id,admission_date,discharge_date,diagnosis,doctor_id,total_cost,\n",
    "              dense_rank()over(partition by diagnosis order by total_cost desc) as denserank from hospital )\n",
    "              select * from cte where denserank =2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "0681871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate the total_cost difference for each diagnosis based on the previous day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "108a0c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "mywindow = Window.partitionBy(\"diagnosis\").orderBy(\"admission_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "0e901513",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_total_cost = hospital_new_df.withColumn(\"next_cost\",lead(\"total_cost\").over(mywindow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "94321177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+---------+----------+---------+-----------------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|doctor_id|total_cost|next_cost|result_difference|\n",
      "+----------+--------------+--------------+-------------+---------+----------+---------+-----------------+\n",
      "|         4|    2022-01-02|    2022-01-08| Heart Attack|      104|  15000.00| 20000.00|           5000.0|\n",
      "|         8|    2022-01-25|    2022-01-01| Heart Attack|      108|  20000.00| 18000.00|          -2000.0|\n",
      "|        13|    2023-01-02|    2023-01-09| Heart Attack|      113|  18000.00| 16000.00|          -2000.0|\n",
      "|        17|    2023-01-08|    2023-01-11| Heart Attack|      117|  16000.00| 21000.00|           5000.0|\n",
      "|        22|    2023-01-12|    2023-01-19| Heart Attack|      122|  21000.00|     null|             null|\n",
      "|         3|    2022-01-12|    2022-01-18|Fractured Arm|      103|   3500.00|  4100.00|            600.0|\n",
      "|        24|    2023-01-01|    2023-01-07|Fractured Arm|      124|   4100.00|  3800.00|           -300.0|\n",
      "|        15|    2023-01-20|    2023-01-28|Fractured Arm|      115|   3800.00|     null|             null|\n",
      "|         9|    2022-01-15|    2022-01-22|Fractured Leg|      109|   6000.00|  6500.00|            500.0|\n",
      "|        19|    2023-01-22|    2023-01-27|Fractured Leg|      119|   6500.00|     null|             null|\n",
      "|         2|    2022-01-05|    2022-01-09| Appendicitis|      102|   7000.00|  7500.00|            500.0|\n",
      "|        10|    2022-01-05|    2022-01-10| Appendicitis|      110|   7500.00|  8000.00|            500.0|\n",
      "|         6|    2022-01-10|    2022-01-15| Appendicitis|      106|   8000.00|  7800.00|           -200.0|\n",
      "|        20|    2023-01-10|    2023-01-16| Appendicitis|      120|   7800.00|  7200.00|           -600.0|\n",
      "|        14|    2023-01-14|    2023-01-18| Appendicitis|      114|   7200.00|     null|             null|\n",
      "|        11|    2022-01-02|    2022-01-05|    Influenza|      111|   2800.00|  2500.00|           -300.0|\n",
      "|         5|    2022-01-05|    2022-01-07|    Influenza|      105|   2500.00|  2700.00|            200.0|\n",
      "|        16|    2023-01-05|    2023-01-11|    Influenza|      116|   2700.00|  2900.00|            200.0|\n",
      "|        21|    2023-01-05|    2023-01-09|    Influenza|      121|   2900.00|  3200.00|            300.0|\n",
      "|        25|    2024-01-10|    2024-01-15|    Influenza|      125|   3200.00|     null|             null|\n",
      "+----------+--------------+--------------+-------------+---------+----------+---------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_final = result_total_cost.withColumn(\"result_difference\",expr(\"next_cost-total_cost\"))\n",
    "result_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "67ebc0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+---------+----------+----------------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|doctor_id|total_cost|resultdifference|\n",
      "+----------+--------------+--------------+-------------+---------+----------+----------------+\n",
      "|         4|    2022-01-02|    2022-01-08| Heart Attack|      104|  15000.00|          5000.0|\n",
      "|         8|    2022-01-25|    2022-01-01| Heart Attack|      108|  20000.00|         -2000.0|\n",
      "|        13|    2023-01-02|    2023-01-09| Heart Attack|      113|  18000.00|         -2000.0|\n",
      "|        17|    2023-01-08|    2023-01-11| Heart Attack|      117|  16000.00|          5000.0|\n",
      "|        22|    2023-01-12|    2023-01-19| Heart Attack|      122|  21000.00|            null|\n",
      "|         3|    2022-01-12|    2022-01-18|Fractured Arm|      103|   3500.00|           600.0|\n",
      "|        24|    2023-01-01|    2023-01-07|Fractured Arm|      124|   4100.00|          -300.0|\n",
      "|        15|    2023-01-20|    2023-01-28|Fractured Arm|      115|   3800.00|            null|\n",
      "|         9|    2022-01-15|    2022-01-22|Fractured Leg|      109|   6000.00|           500.0|\n",
      "|        19|    2023-01-22|    2023-01-27|Fractured Leg|      119|   6500.00|            null|\n",
      "|         2|    2022-01-05|    2022-01-09| Appendicitis|      102|   7000.00|           500.0|\n",
      "|        10|    2022-01-05|    2022-01-10| Appendicitis|      110|   7500.00|           500.0|\n",
      "|         6|    2022-01-10|    2022-01-15| Appendicitis|      106|   8000.00|          -200.0|\n",
      "|        20|    2023-01-10|    2023-01-16| Appendicitis|      120|   7800.00|          -600.0|\n",
      "|        14|    2023-01-14|    2023-01-18| Appendicitis|      114|   7200.00|            null|\n",
      "|        11|    2022-01-02|    2022-01-05|    Influenza|      111|   2800.00|          -300.0|\n",
      "|         5|    2022-01-05|    2022-01-07|    Influenza|      105|   2500.00|           200.0|\n",
      "|        16|    2023-01-05|    2023-01-11|    Influenza|      116|   2700.00|           200.0|\n",
      "|        21|    2023-01-05|    2023-01-09|    Influenza|      121|   2900.00|           300.0|\n",
      "|        25|    2024-01-10|    2024-01-15|    Influenza|      125|   3200.00|            null|\n",
      "+----------+--------------+--------------+-------------+---------+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" with cte as (select patient_id,admission_date,discharge_date,diagnosis,doctor_id,total_cost,\n",
    "              lead(total_cost)over(partition by diagnosis order by admission_date) as next_cost from hospital )\n",
    "              select patient_id,admission_date,discharge_date,diagnosis,doctor_id,total_cost\n",
    "              ,(next_cost - total_cost) as resultdifference from cte\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "7b04a840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT doctor_id)|\n",
      "+-------------------------+\n",
      "|                       25|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(distinct(doctor_id)) from hospital\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "96cf1788",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pivot the data based on admission_year and diagnosis and count of patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "9fc3082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "99587026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+----+----+\n",
      "|    diagnosis|2022|2023|2024|\n",
      "+-------------+----+----+----+\n",
      "| Heart Attack|   2|   3|   0|\n",
      "|Fractured Arm|   1|   2|   0|\n",
      "|Fractured Leg|   1|   1|   0|\n",
      "| Appendicitis|   3|   2|   0|\n",
      "|    Influenza|   2|   2|   1|\n",
      "|    Pneumonia|   3|   2|   0|\n",
      "+-------------+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hospital_new1_df = hospital_new_df.select(\"diagnosis\", year(\"admission_date\").alias(\"year\"))\n",
    "\n",
    "# Group by diagnosis and pivot on year, count the number of entries, and fill null values with 0\n",
    "hospital_new1_df.groupBy(\"diagnosis\") \\\n",
    "    .pivot(\"year\") \\\n",
    "    .count() \\\n",
    "    .na.fill(0) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "39271652",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pivot with spark sql is not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "ccf88942",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" with cte as (SELECT diagnosis,\n",
    "                   YEAR(admission_date) AS year,\n",
    "                   COUNT(*) AS count\n",
    "                   FROM hospital\n",
    "                   GROUP BY diagnosis, YEAR(admission_date))\n",
    "                 select diagnosis,pivot(year),count from cte\n",
    "          \"\"\").show()\n",
    "spark.sql(\"\"\"\n",
    "    SELECT diagnosis, \n",
    "           PIVOT(\n",
    "           COALESCE(count, 0) FOR year IN('2022','2023','2024')\n",
    "    FROM hospital_summary\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "40db9d65",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\nmissing 'IN' at '('(line 4, pos 44)\n\n== SQL ==\nselect diagnosis,'2022','2023','2024' \n            from hospital\n              PIVOT(\n                  count(patient_id) FOR year(admission_date) IN ('2022','2023','2024')\n--------------------------------------------^^^\n                  )\n         \n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-215-234dbb42ea32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                   \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatient_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mFOR\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madmission_date\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mIN\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'2022'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2023'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2024'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                   )\n\u001b[0;32m----> 6\u001b[0;31m          \"\"\").show()\n\u001b[0m",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \"\"\"\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParseException\u001b[0m: \nmissing 'IN' at '('(line 4, pos 44)\n\n== SQL ==\nselect diagnosis,'2022','2023','2024' \n            from hospital\n              PIVOT(\n                  count(patient_id) FOR year(admission_date) IN ('2022','2023','2024')\n--------------------------------------------^^^\n                  )\n         \n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select diagnosis,'2022','2023','2024' \n",
    "            from hospital\n",
    "              PIVOT(\n",
    "                  count(patient_id) FOR year(admission_date) IN ('2022','2023','2024')\n",
    "                  )\n",
    "         \"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa5acfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ff5ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416d8ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
