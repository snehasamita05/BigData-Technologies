{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60a826d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession. \\\n",
    "builder. \\\n",
    "config('spark.ui.port','0'). \\\n",
    "config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "enableHiveSupport(). \\\n",
    "master('yarn'). \\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f726e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://g01.itversity.com:38495\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f66d1473710>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fef27bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61a12a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_schema = StructType([\n",
    "                            StructField(\"customer_id\",LongType()),\n",
    "                            StructField(\"fullname\",\n",
    "                                        StructType([StructField(\"firstname\",StringType()),\n",
    "                                                    StructField(\"lastname\", StringType())\n",
    "                                                   ])\n",
    "                                       ),\n",
    "                            StructField(\"city\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50cae220",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"json\").schema(customer_schema).load(\"/public/trendytech/datasets/customer_nested/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3350602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+---------+\n",
      "|customer_id|        fullname|     city|\n",
      "+-----------+----------------+---------+\n",
      "|          2|    {ram, kumar}|hyderabad|\n",
      "|          3|{vijay, shankar}|     pune|\n",
      "|          1| {sumit, mittal}|bangalore|\n",
      "+-----------+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56a9ba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_list = [( 1, (\"sumit\",\"mittal\"),\"bangalore\"),\n",
    "                (2,(\"sneha\", \"samita\"),\"sweden\"),\n",
    "                (3,(\"mrinal\",\"kumar\"),\"sweden\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9caeae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddlschema = \"customer_id long, name struct<firstname: string, lastname: string>, city string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2d634b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(customer_list,ddlschema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "437801e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+---------+\n",
      "|customer_id|           name|     city|\n",
      "+-----------+---------------+---------+\n",
      "|          1|{sumit, mittal}|bangalore|\n",
      "|          2|{sneha, samita}|   sweden|\n",
      "|          3|{mrinal, kumar}|   sweden|\n",
      "+-----------+---------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11437cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b72c13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_schema = StructType([\n",
    "                            StructField(\"customer_id\", StringType()),\n",
    "                            StructField(\"name\",StructType([\n",
    "                                StructField(\"firstname\",StringType()),\n",
    "                                StructField(\"lastname\", StringType())\n",
    "                                                        ])),\n",
    "                            StructField(\"city\",StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad0a3c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(customer_list,customer_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2933bd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+---------+\n",
      "|customer_id|           name|     city|\n",
      "+-----------+---------------+---------+\n",
      "|          1|{sumit, mittal}|bangalore|\n",
      "|          2|{sneha, samita}|   sweden|\n",
      "|          3|{mrinal, kumar}|   sweden|\n",
      "+-----------+---------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62812a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 47974)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 351, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 364, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 724, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/accumulators.py\", line 262, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/accumulators.py\", line 239, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4931df25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
