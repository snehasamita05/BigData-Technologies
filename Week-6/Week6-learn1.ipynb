{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38833381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession. \\\n",
    "builder. \\\n",
    "config('spark.ui.port','0'). \\\n",
    "config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "enableHiveSupport(). \\\n",
    "master('yarn'). \\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d49f7475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://g01.itversity.com:40897\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcecc775748>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8290a6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.\\\n",
    "format(\"csv\").\\\n",
    "option(\"header\",\"true\").\\\n",
    "option(\"inferSchema\", \"True\").\\\n",
    "load(\"/public/yelp-dataset/yelp_user.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "172e85a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------------+-------------+--------------------+------+-----+----+----+-----+-------------+--------------+---------------+------------------+---------------+---------------+---------------+----------------+---------------+----------------+-----------------+-----------------+\n",
      "|             user_id|  name|review_count|yelping_since|             friends|useful|funny|cool|fans|elite|average_stars|compliment_hot|compliment_more|compliment_profile|compliment_cute|compliment_list|compliment_note|compliment_plain|compliment_cool|compliment_funny|compliment_writer|compliment_photos|\n",
      "+--------------------+------+------------+-------------+--------------------+------+-----+----+----+-----+-------------+--------------+---------------+------------------+---------------+---------------+---------------+----------------+---------------+----------------+-----------------+-----------------+\n",
      "|JJ-aSuM4pCFPdkfoZ...| Chris|          10|   2013-09-24|0njfJmB-7n84DlIgU...|     0|    0|   0|   0| None|          3.7|             0|              0|                 0|              0|              0|              0|               0|              0|               0|                0|                0|\n",
      "|uUzsFQn_6cXDh6rPN...| Tiffy|           1|   2017-03-02|                None|     0|    0|   0|   0| None|          2.0|             0|              0|                 0|              0|              0|              0|               0|              0|               0|                0|                0|\n",
      "|mBneaEEH5EMyxaVyq...|  Mark|           6|   2015-03-13|                None|     0|    0|   0|   0| None|         4.67|             0|              0|                 0|              0|              0|              0|               0|              0|               0|                0|                0|\n",
      "|W5mJGs-dcDWRGEhAz...|Evelyn|           3|   2016-09-08|                None|     0|    0|   0|   0| None|         4.67|             0|              0|                 0|              0|              0|              0|               0|              0|               0|                0|                0|\n",
      "+--------------------+------+------------+-------------+--------------------+------+-----+----+----+-----+-------------+--------------+---------------+------------------+---------------+---------------+---------------+----------------+---------------+----------------+-----------------+-----------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd3e3b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- review_count: integer (nullable = true)\n",
      " |-- yelping_since: string (nullable = true)\n",
      " |-- friends: string (nullable = true)\n",
      " |-- useful: integer (nullable = true)\n",
      " |-- funny: integer (nullable = true)\n",
      " |-- cool: integer (nullable = true)\n",
      " |-- fans: integer (nullable = true)\n",
      " |-- elite: string (nullable = true)\n",
      " |-- average_stars: double (nullable = true)\n",
      " |-- compliment_hot: integer (nullable = true)\n",
      " |-- compliment_more: integer (nullable = true)\n",
      " |-- compliment_profile: integer (nullable = true)\n",
      " |-- compliment_cute: integer (nullable = true)\n",
      " |-- compliment_list: integer (nullable = true)\n",
      " |-- compliment_note: integer (nullable = true)\n",
      " |-- compliment_plain: integer (nullable = true)\n",
      " |-- compliment_cool: integer (nullable = true)\n",
      " |-- compliment_funny: integer (nullable = true)\n",
      " |-- compliment_writer: integer (nullable = true)\n",
      " |-- compliment_photos: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3897417",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.\\\n",
    "format(\"csv\").\\\n",
    "option(\"header\",\"true\").\\\n",
    "option(\"inferSchema\", \"True\").\\\n",
    "option(\"samplingRatio\", .1).\\\n",
    "load(\"/public/yelp-dataset/yelp_user.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "715aa704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- review_count: integer (nullable = true)\n",
      " |-- yelping_since: string (nullable = true)\n",
      " |-- friends: string (nullable = true)\n",
      " |-- useful: integer (nullable = true)\n",
      " |-- funny: integer (nullable = true)\n",
      " |-- cool: integer (nullable = true)\n",
      " |-- fans: integer (nullable = true)\n",
      " |-- elite: string (nullable = true)\n",
      " |-- average_stars: double (nullable = true)\n",
      " |-- compliment_hot: integer (nullable = true)\n",
      " |-- compliment_more: integer (nullable = true)\n",
      " |-- compliment_profile: integer (nullable = true)\n",
      " |-- compliment_cute: integer (nullable = true)\n",
      " |-- compliment_list: integer (nullable = true)\n",
      " |-- compliment_note: integer (nullable = true)\n",
      " |-- compliment_plain: integer (nullable = true)\n",
      " |-- compliment_cool: integer (nullable = true)\n",
      " |-- compliment_funny: integer (nullable = true)\n",
      " |-- compliment_writer: integer (nullable = true)\n",
      " |-- compliment_photos: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "556430b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.\\\n",
    "format(\"csv\").\\\n",
    "load(\"/public/trendytech/datasets/orders_sample1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bb11f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+---------------+\n",
      "|_c0|       _c1|  _c2|            _c3|\n",
      "+---+----------+-----+---------------+\n",
      "|  1|2013-07-25|11599|         CLOSED|\n",
      "|  2|2013-07-25|  256|PENDING_PAYMENT|\n",
      "|  3|2013-07-25|12111|       COMPLETE|\n",
      "|  4|2013-07-25| 8827|         CLOSED|\n",
      "+---+----------+-----+---------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b75b92eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5600a66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_schema = 'order_id long, order_date date, cust_id long, order_status string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ad4f65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.\\\n",
    "format(\"csv\").\\\n",
    "schema(orders_schema).\\\n",
    "load(\"/public/trendytech/datasets/orders_sample1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08608033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+---------------+\n",
      "|order_id|order_date|cust_id|   order_status|\n",
      "+--------+----------+-------+---------------+\n",
      "|       1|2013-07-25|  11599|         CLOSED|\n",
      "|       2|2013-07-25|    256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25|  12111|       COMPLETE|\n",
      "|       4|2013-07-25|   8827|         CLOSED|\n",
      "|       5|2013-07-25|  11318|       COMPLETE|\n",
      "|       6|2013-07-25|   7130|       COMPLETE|\n",
      "|       7|2013-07-25|   4530|       COMPLETE|\n",
      "|       8|2013-07-25|   2911|     PROCESSING|\n",
      "|       9|2013-07-25|   5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25|   5648|PENDING_PAYMENT|\n",
      "+--------+----------+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "453c0764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- cust_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b4983d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_schema = 'order_id long, order_date date, cust_id long, order_status long'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "446032ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.\\\n",
    "format(\"csv\").\\\n",
    "schema(orders_schema).\\\n",
    "load(\"/public/trendytech/datasets/orders_sample1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04c75027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+------------+\n",
      "|order_id|order_date|cust_id|order_status|\n",
      "+--------+----------+-------+------------+\n",
      "|       1|2013-07-25|  11599|        null|\n",
      "|       2|2013-07-25|    256|        null|\n",
      "|       3|2013-07-25|  12111|        null|\n",
      "|       4|2013-07-25|   8827|        null|\n",
      "|       5|2013-07-25|  11318|        null|\n",
      "|       6|2013-07-25|   7130|        null|\n",
      "|       7|2013-07-25|   4530|        null|\n",
      "|       8|2013-07-25|   2911|        null|\n",
      "|       9|2013-07-25|   5657|        null|\n",
      "|      10|2013-07-25|   5648|        null|\n",
      "+--------+----------+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1702772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6da4636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_schema_struct =StructType([\n",
    "                                StructField('orderid', LongType()),\n",
    "                                StructField('orderdate',DateType()),\n",
    "                                StructField('customerid',IntegerType()),\n",
    "                                StructField('orderStatus',StringType())\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebb4c27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").schema(order_schema_struct).load(\"/public/trendytech/datasets/orders_sample1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98df1f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+---------------+\n",
      "|orderid| orderdate|customerid|    orderStatus|\n",
      "+-------+----------+----------+---------------+\n",
      "|      1|2013-07-25|     11599|         CLOSED|\n",
      "|      2|2013-07-25|       256|PENDING_PAYMENT|\n",
      "|      3|2013-07-25|     12111|       COMPLETE|\n",
      "|      4|2013-07-25|      8827|         CLOSED|\n",
      "|      5|2013-07-25|     11318|       COMPLETE|\n",
      "|      6|2013-07-25|      7130|       COMPLETE|\n",
      "|      7|2013-07-25|      4530|       COMPLETE|\n",
      "|      8|2013-07-25|      2911|     PROCESSING|\n",
      "|      9|2013-07-25|      5657|PENDING_PAYMENT|\n",
      "|     10|2013-07-25|      5648|PENDING_PAYMENT|\n",
      "+-------+----------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d6c3d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- orderid: long (nullable = true)\n",
      " |-- orderdate: date (nullable = true)\n",
      " |-- customerid: integer (nullable = true)\n",
      " |-- orderStatus: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65c5855d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,07-25-2013,11599,CLOSED\n",
      "2,07-25-2013,256,PENDING_PAYMENT\n",
      "3,07-25-2013,12111,COMPLETE\n",
      "4,07-25-2013,8827,CLOSED\n",
      "5,07-25-2013,11318,COMPLETE\n",
      "6,07-25-2013,7130,COMPLETE\n",
      "7,07-25-2013,4530,COMPLETE\n",
      "8,07-25-2013,2911,PROCESSING\n",
      "9,07-25-2013,5657,PENDING_PAYMENT\n",
      "10,07-25-2013,5648,PENDING_PAYMENT\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -cat /public/trendytech/datasets/orders_sample2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cff80ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_schema = 'order_id long, order_date date, cust_id long, order_status string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a82eb26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.\\\n",
    "format(\"csv\").\\\n",
    "schema(orders_schema).\\\n",
    "option(\"dateformat\",\"mm-dd-yyyy\").\\\n",
    "load(\"/public/trendytech/datasets/orders_sample2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "891cadbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+---------------+\n",
      "|order_id|order_date|cust_id|   order_status|\n",
      "+--------+----------+-------+---------------+\n",
      "|       1|2013-01-25|  11599|         CLOSED|\n",
      "|       2|2013-01-25|    256|PENDING_PAYMENT|\n",
      "|       3|2013-01-25|  12111|       COMPLETE|\n",
      "|       4|2013-01-25|   8827|         CLOSED|\n",
      "|       5|2013-01-25|  11318|       COMPLETE|\n",
      "|       6|2013-01-25|   7130|       COMPLETE|\n",
      "|       7|2013-01-25|   4530|       COMPLETE|\n",
      "|       8|2013-01-25|   2911|     PROCESSING|\n",
      "|       9|2013-01-25|   5657|PENDING_PAYMENT|\n",
      "|      10|2013-01-25|   5648|PENDING_PAYMENT|\n",
      "+--------+----------+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97e37374",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_schema = 'order_id long, order_date string, cust_id long, order_status string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea62a735",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.\\\n",
    "format(\"csv\").\\\n",
    "schema(orders_schema).\\\n",
    "load(\"/public/trendytech/datasets/orders_sample2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f4fc328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+---------------+\n",
      "|order_id|order_date|cust_id|   order_status|\n",
      "+--------+----------+-------+---------------+\n",
      "|       1|07-25-2013|  11599|         CLOSED|\n",
      "|       2|07-25-2013|    256|PENDING_PAYMENT|\n",
      "|       3|07-25-2013|  12111|       COMPLETE|\n",
      "|       4|07-25-2013|   8827|         CLOSED|\n",
      "|       5|07-25-2013|  11318|       COMPLETE|\n",
      "|       6|07-25-2013|   7130|       COMPLETE|\n",
      "|       7|07-25-2013|   4530|       COMPLETE|\n",
      "|       8|07-25-2013|   2911|     PROCESSING|\n",
      "|       9|07-25-2013|   5657|PENDING_PAYMENT|\n",
      "|      10|07-25-2013|   5648|PENDING_PAYMENT|\n",
      "+--------+----------+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ca83bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- cust_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57eead2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe29894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.withColumn(\"order_date\",to_date(\"order_date\",\"mm-dd-yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "867a00b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+---------------+\n",
      "|order_id|order_date|cust_id|   order_status|\n",
      "+--------+----------+-------+---------------+\n",
      "|       1|2013-01-25|  11599|         CLOSED|\n",
      "|       2|2013-01-25|    256|PENDING_PAYMENT|\n",
      "|       3|2013-01-25|  12111|       COMPLETE|\n",
      "|       4|2013-01-25|   8827|         CLOSED|\n",
      "|       5|2013-01-25|  11318|       COMPLETE|\n",
      "|       6|2013-01-25|   7130|       COMPLETE|\n",
      "|       7|2013-01-25|   4530|       COMPLETE|\n",
      "|       8|2013-01-25|   2911|     PROCESSING|\n",
      "|       9|2013-01-25|   5657|PENDING_PAYMENT|\n",
      "|      10|2013-01-25|   5648|PENDING_PAYMENT|\n",
      "+--------+----------+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9c6e534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- cust_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7bd7ae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_extra = df.withColumn(\"order_date_new\",to_date(\"order_date\",\"mm-dd-yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ba1832d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- cust_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_date_new: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new_extra.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9ed10160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,2013-07-25,11599,CLOSED\n",
      "2,2013-07-25,256,PENDING_PAYMENT\n",
      "3,2013-07-25,12111,COMPLETE\n",
      "4,2013-07-25,8827,CLOSED\n",
      "5,2013-07-25,11318,COMPLETE\n",
      "6,2013-07-25,7130,COMPLETE\n",
      "7,2013-07-25,error,COMPLETE\n",
      "8,2013-07-25,2911,PROCESSING\n",
      "9,2013-07-25,unknown,PENDING_PAYMENT\n",
      "10,2013-07-25,5648,PENDING_PAYMENT\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -cat /public/trendytech/datasets/orders_sample3.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "897ec637",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_schema = 'order_id long, order_date string, cust_id long, order_status string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "72cc973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.\\\n",
    "format(\"csv\").\\\n",
    "schema(orders_schema).\\\n",
    "load(\"/public/trendytech/datasets/orders_sample3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e2573049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+---------------+\n",
      "|order_id|order_date|cust_id|   order_status|\n",
      "+--------+----------+-------+---------------+\n",
      "|       1|2013-07-25|  11599|         CLOSED|\n",
      "|       2|2013-07-25|    256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25|  12111|       COMPLETE|\n",
      "|       4|2013-07-25|   8827|         CLOSED|\n",
      "|       5|2013-07-25|  11318|       COMPLETE|\n",
      "|       6|2013-07-25|   7130|       COMPLETE|\n",
      "|       7|2013-07-25|   null|       COMPLETE|\n",
      "|       8|2013-07-25|   2911|     PROCESSING|\n",
      "|       9|2013-07-25|   null|PENDING_PAYMENT|\n",
      "|      10|2013-07-25|   5648|PENDING_PAYMENT|\n",
      "+--------+----------+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e228aad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_schema = 'order_id long, order_date string, cust_id long, order_status string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0231fefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.\\\n",
    "format(\"csv\").\\\n",
    "schema(orders_schema).\\\n",
    "option(\"mode\", \"failfast\").\\\n",
    "load(\"/public/trendytech/datasets/orders_sample3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "567e8510",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.\\\n",
    "format(\"csv\").\\\n",
    "schema(orders_schema).\\\n",
    "option(\"mode\", \"dropmalformed\").\\\n",
    "load(\"/public/trendytech/datasets/orders_sample3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dc35c525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+---------------+\n",
      "|order_id|order_date|cust_id|   order_status|\n",
      "+--------+----------+-------+---------------+\n",
      "|       1|2013-07-25|  11599|         CLOSED|\n",
      "|       2|2013-07-25|    256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25|  12111|       COMPLETE|\n",
      "|       4|2013-07-25|   8827|         CLOSED|\n",
      "|       5|2013-07-25|  11318|       COMPLETE|\n",
      "|       6|2013-07-25|   7130|       COMPLETE|\n",
      "|       8|2013-07-25|   2911|     PROCESSING|\n",
      "|      10|2013-07-25|   5648|PENDING_PAYMENT|\n",
      "+--------+----------+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f6d88a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d933417b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>namespace</th></tr>\n",
       "<tr><td>itv017244_assignm...</td></tr>\n",
       "<tr><td>itv017244_retail</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+\n",
       "|           namespace|\n",
       "+--------------------+\n",
       "|itv017244_assignm...|\n",
       "|    itv017244_retail|\n",
       "+--------------------+"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"show databases\").filter(\"namespace like '%itv017244%'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8b79a793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use itv017244_retail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8261a06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+-----------+\n",
      "|        database| tableName|isTemporary|\n",
      "+----------------+----------+-----------+\n",
      "|itv017244_retail|    orders|      false|\n",
      "|itv017244_retail|orders_ext|      false|\n",
      "+----------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7ce51f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = spark.sql(\"select * from itv017244_retail.orders_ext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b88228d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o218.showString.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.lang.Thread.run(Thread.java:750)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:118)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1506)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.buildReader(CSVFileFormat.scala:102)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:130)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:121)\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:170)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:407)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:398)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:485)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:321)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:439)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-edf8d6113b07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0morders_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \"\"\"\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o218.showString.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.lang.Thread.run(Thread.java:750)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:118)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1506)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.buildReader(CSVFileFormat.scala:102)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:130)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:121)\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:170)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:407)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:398)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:485)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:321)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:439)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a297f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.table(\"itv017244_retail.orders_ext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72e2aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83f7df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af133ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs  -cat /public/trendytech/retail_db/orders/part-00000 |head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388194fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_list = [(1,'2013-07-25 00:00:00.0',11599,'CLOSED'),\n",
    "                 (2,'2013-07-25 00:00:00.0',256,'PENDING_PAYMENT'),\n",
    "                 (3,'2013-07-25 00:00:00.0',12111,'COMPLETE')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc34fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_raw_df = spark.createDataFrame(orders_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74768ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_raw_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f740183",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_raw_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f38545",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_raw_df = spark.createDataFrame(orders_list).toDF('order_id','order_date','customer_id','order_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c651de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_raw_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba386da",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_schema = [\"order_id\",\"order_date\",\"cust_id\",\"order_status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2294db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(orders_list,orders_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a247609d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666f4695",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_schema = 'order_id long, order_date string, cust_id int, order_status string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49073444",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(orders_list,orders_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc1c08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096ceda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfee5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee66178",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.withColumn(\"order_date\",to_timestamp(\"order_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0061406",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a722487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc75233",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_rdd = spark.sparkContext.textFile(\"/public/trendytech/retail_db/orders/part-00000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4ec4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_rdd.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc6625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_orders_rdd = orders_rdd.map(lambda x : (int(x.split(\",\")[0]),x.split(\",\")[1],int(x.split(\",\")[2]),x.split(\",\")[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19042485",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_orders_rdd.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bb5f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_schema = 'order_id long, order_date string, cust_id int, order_status string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c360a7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(new_orders_rdd,orders_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ff581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ceb1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206d93ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = spark.createDataFrame(new_orders_rdd).toDF('order_id','order_date','cust_id','order_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230c0277",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f39d0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_schema = 'order_id long, order_date string, cust_id int, order_status string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f233d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = new_orders_rdd.toDF(orders_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8ad683",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411db6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aca683",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bcdbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -ls /public/trendytech/datasets/customer/nested/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdb05ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaddl = \"customer_id long, fullname struct<firstname: string,lastname: string>,city string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991d80d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"json\").schema(schemaddl).load(\"/public/trendytech/datasets/customer_nested/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b88d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da535e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b0da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92326163",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_schema = StructType([\n",
    "                            StructField(\"customer_id\",LongType()),\n",
    "                            StructField(\"fullname\",\n",
    "                                        StructType([StructField(\"firstname\",StringType()),\n",
    "                                                    StructField(\"lastname\", StringType())\n",
    "                                                   ])\n",
    "                                       ),\n",
    "                            StructField(\"city\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc59e119",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"json\").schema(customer_schema).load(\"/public/trendytech/datasets/customer_nested/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b83f9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb17afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7c48fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
