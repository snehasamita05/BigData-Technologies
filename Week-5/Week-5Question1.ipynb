{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e822c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "spark= SparkSession. \\\n",
    "builder. \\\n",
    "config('spark.ui.port','0'). \\\n",
    "config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "enableHiveSupport(). \\\n",
    "master('yarn'). \\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b72c801c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://g01.itversity.com:44961\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3d6c569390>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c45cb3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create database if not exists itv017244_assignments_week5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13b53c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|0000000000000_msdian|\n",
      "|0000000000000_nav...|\n",
      "|0000000009874_retail|\n",
      "|          00000_2_db|\n",
      "|       00000assg5_db|\n",
      "|     00000assign5_db|\n",
      "|00000assign7_itv0...|\n",
      "|0000_0spark_tabbl...|\n",
      "| 0000_cache_spark111|\n",
      "|        0000a5a5a5a5|\n",
      "|   0001_av_ivy_tesco|\n",
      "|          001_retail|\n",
      "|        003402_hive1|\n",
      "|    005198_ivy_tesco|\n",
      "|    005212_ivy_tesco|\n",
      "| 005222_ivy_practice|\n",
      "| 005260_ivy_database|\n",
      "|005876_week5_assi...|\n",
      "|       005933_retail|\n",
      "|        006539_week5|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "971e707d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|itv017244_assignm...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").filter(\"namespace = 'itv017244_assignments_week5'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "abe6ef09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use itv017244_assignments_week5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f89013f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----------+\n",
      "|            database|tableName|isTemporary|\n",
      "+--------------------+---------+-----------+\n",
      "|itv017244_assignm...|   orders|      false|\n",
      "|                    |groceries|       true|\n",
      "+--------------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3da8a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14eb3418",
   "metadata": {},
   "outputs": [],
   "source": [
    "groceries_df = spark.read.\\\n",
    "format(\"csv\").\\\n",
    "option(\"header\",\"true\").\\\n",
    "option(\"inferSchema\", \"True\").\\\n",
    "load(\"/public/trendytech/groceries.csv*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fed61dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------+----------+--------+\n",
      "|order_id|location|   item|order_date|quantity|\n",
      "+--------+--------+-------+----------+--------+\n",
      "|      o1| Seattle|Bananas|01/01/2017|       7|\n",
      "|      o2|    Kent| Apples|02/01/2017|      20|\n",
      "|      o3|Bellevue|Flowers|02/01/2017|      10|\n",
      "|      o4| Redmond|   Meat|03/01/2017|      40|\n",
      "+--------+--------+-------+----------+--------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "groceries_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "21571aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "groceries_df.createOrReplaceTempView(\"groceries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2262ad2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>order_id</th><th>location</th><th>item</th><th>order_date</th><th>quantity</th></tr>\n",
       "<tr><td>o1</td><td>Seattle</td><td>Bananas</td><td>01/01/2017</td><td>7</td></tr>\n",
       "<tr><td>o2</td><td>Kent</td><td>Apples</td><td>02/01/2017</td><td>20</td></tr>\n",
       "<tr><td>o3</td><td>Bellevue</td><td>Flowers</td><td>02/01/2017</td><td>10</td></tr>\n",
       "<tr><td>o4</td><td>Redmond</td><td>Meat</td><td>03/01/2017</td><td>40</td></tr>\n",
       "<tr><td>o5</td><td>Seattle</td><td>Potatoes</td><td>04/01/2017</td><td>9</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+--------+--------+----------+--------+\n",
       "|order_id|location|    item|order_date|quantity|\n",
       "+--------+--------+--------+----------+--------+\n",
       "|      o1| Seattle| Bananas|01/01/2017|       7|\n",
       "|      o2|    Kent|  Apples|02/01/2017|      20|\n",
       "|      o3|Bellevue| Flowers|02/01/2017|      10|\n",
       "|      o4| Redmond|    Meat|03/01/2017|      40|\n",
       "|      o5| Seattle|Potatoes|04/01/2017|       9|\n",
       "+--------+--------+--------+----------+--------+"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from groceries limit 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4197917e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- item: string (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "groceries_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e210c917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create table itv017244_assignments_week5.groceries(order_id string, location string, item string,order_date string, quantity integer)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f3c74447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----------+\n",
      "|            database|tableName|isTemporary|\n",
      "+--------------------+---------+-----------+\n",
      "|itv017244_assignm...|groceries|      false|\n",
      "|itv017244_assignm...|   orders|      false|\n",
      "|                    |groceries|       true|\n",
      "+--------------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e441b532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-----------------------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                                      |comment|\n",
      "+----------------------------+-----------------------------------------------------------------------------------------------+-------+\n",
      "|order_id                    |string                                                                                         |null   |\n",
      "|location                    |string                                                                                         |null   |\n",
      "|item                        |string                                                                                         |null   |\n",
      "|order_date                  |string                                                                                         |null   |\n",
      "|quantity                    |int                                                                                            |null   |\n",
      "|                            |                                                                                               |       |\n",
      "|# Detailed Table Information|                                                                                               |       |\n",
      "|Database                    |itv017244_assignments_week5                                                                    |       |\n",
      "|Table                       |groceries                                                                                      |       |\n",
      "|Owner                       |itv017244                                                                                      |       |\n",
      "|Created Time                |Thu Feb 20 18:32:23 EST 2025                                                                   |       |\n",
      "|Last Access                 |UNKNOWN                                                                                        |       |\n",
      "|Created By                  |Spark 3.1.2                                                                                    |       |\n",
      "|Type                        |MANAGED                                                                                        |       |\n",
      "|Provider                    |hive                                                                                           |       |\n",
      "|Table Properties            |[transient_lastDdlTime=1740094343]                                                             |       |\n",
      "|Location                    |hdfs://m01.itversity.com:9000/user/itv017244/warehouse/itv017244_assignments_week5.db/groceries|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                                             |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.TextInputFormat                                                       |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat                                     |       |\n",
      "+----------------------------+-----------------------------------------------------------------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe extended itv017244_assignments_week5.groceries\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e98c1073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----+----------+--------+\n",
      "|order_id|location|item|order_date|quantity|\n",
      "+--------+--------+----+----------+--------+\n",
      "+--------+--------+----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from itv017244_assignments_week5.groceries limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cff2928c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"insert into itv017244_assignments_week5.groceries select * from groceries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "08fe9a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create table itv017244_assignments_week5.groceries_EXT(order_id string, location string, item string,order_date string, quantity integer) using csv location '/public/trendytech/groceries.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0e8e5e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------+----------+--------+\n",
      "|order_id|location|   item|order_date|quantity|\n",
      "+--------+--------+-------+----------+--------+\n",
      "|order_id|location|   item|order_date|    null|\n",
      "|      o1| Seattle|Bananas|01/01/2017|       7|\n",
      "|      o2|    Kent| Apples|02/01/2017|      20|\n",
      "|      o3|Bellevue|Flowers|02/01/2017|      10|\n",
      "|      o4| Redmond|   Meat|03/01/2017|      40|\n",
      "+--------+--------+-------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from itv017244_assignments_week5.groceries_EXT limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e460e9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop table itv017244_assignments_week5.groceries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c46b8b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-----------+\n",
      "|            database|    tableName|isTemporary|\n",
      "+--------------------+-------------+-----------+\n",
      "|itv017244_assignm...|groceries_ext|      false|\n",
      "|itv017244_assignm...|       orders|      false|\n",
      "|                    |    groceries|       true|\n",
      "+--------------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a9e66f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop table itv017244_assignments_week5.groceries_EXT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3794a532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----------+\n",
      "|            database|tableName|isTemporary|\n",
      "+--------------------+---------+-----------+\n",
      "|itv017244_assignm...|   orders|      false|\n",
      "|                    |groceries|       true|\n",
      "+--------------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59ccb125",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table or view 'orders' already exists in database 'itv017244_assignments_week5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-7d5fcd2c8d54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"create table itv017244_assignments_week5.orders(order_id integer,order_date string,customer_id integer,order_status string)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \"\"\"\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table or view 'orders' already exists in database 'itv017244_assignments_week5'"
     ]
    }
   ],
   "source": [
    "spark.sql(\"create table itv017244_assignments_week5.orders(order_id integer,order_date string,customer_id integer,order_status string)\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "11cf4472",
   "metadata": {},
   "source": [
    "orders_df = spark.read.json(\"/public/trendytech/datasets/orders.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0350a41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "891d7d28",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'orders_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-edf8d6113b07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0morders_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'orders_df' is not defined"
     ]
    }
   ],
   "source": [
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbae40f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "496d8ed9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'orders_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-7e9e9150b066>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0morders_df_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morders_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"order_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"order_date\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"customer_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"order_status\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'orders_df' is not defined"
     ]
    }
   ],
   "source": [
    "orders_df_new = orders_df.select(\"order_id\", \"order_date\", \"customer_id\",\"order_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8671b1ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table or view not found: orders_df; line 1 pos 61;\n'InsertIntoStatement 'UnresolvedRelation [itv017244_assignments_week5, orders], [], false, false, false\n+- 'Project [*]\n   +- 'UnresolvedRelation [orders_df], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-38dabdae7b64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"insert into itv017244_assignments_week5.orders select * from orders_df\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \"\"\"\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table or view not found: orders_df; line 1 pos 61;\n'InsertIntoStatement 'UnresolvedRelation [itv017244_assignments_week5, orders], [], false, false, false\n+- 'Project [*]\n   +- 'UnresolvedRelation [orders_df], [], false\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"insert into itv017244_assignments_week5.orders select * from orders_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a91e36c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
