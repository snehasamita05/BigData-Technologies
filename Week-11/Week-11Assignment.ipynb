{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f54179d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port','0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    config('spark.shuffle.useOldFetchProtocol','true'). \\\n",
    "    config(\"spark.dynamicAllocation.enabled\", \"True\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bedaabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f060fdc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'True'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.dynamicAllocation.enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98556d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_schema = 'order_id long, order_date string , customer_id long, order_status string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c20233b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   3 itv017244 supergroup      3.5 G 2025-03-17 05:53 /user/itv017244/week11Assignment/orders.csv\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs  -ls -h /user/itv017244/week11Assignment/orders.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccb25d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df = spark.read.\\\n",
    "format(\"csv\").\\\n",
    "schema(order_schema).\\\n",
    "load(\"/user/itv017244/week11Assignment/orders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eca3674",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df.createOrReplaceTempView(\"order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aaa99712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+---------+\n",
      "|   order_status|MonthNum|NoOfOrder|\n",
      "+---------------+--------+---------+\n",
      "|        ON_HOLD|      05|   386250|\n",
      "|       COMPLETE|      07|  2417500|\n",
      "|         CLOSED|      09|   845000|\n",
      "|         CLOSED|      01|   791250|\n",
      "|SUSPECTED_FRAUD|      09|   185000|\n",
      "|       CANCELED|      02|   156250|\n",
      "|SUSPECTED_FRAUD|      06|   163750|\n",
      "|        PENDING|      07|   847500|\n",
      "|       COMPLETE|      08|  2350000|\n",
      "|       CANCELED|      08|   156250|\n",
      "|SUSPECTED_FRAUD|      10|   135000|\n",
      "|        ON_HOLD|      04|   416250|\n",
      "| PAYMENT_REVIEW|      07|    91250|\n",
      "|SUSPECTED_FRAUD|      07|   167500|\n",
      "|SUSPECTED_FRAUD|      01|   163750|\n",
      "|        ON_HOLD|      03|   403750|\n",
      "|       CANCELED|      06|   130000|\n",
      "|       COMPLETE|      06|  2246250|\n",
      "|PENDING_PAYMENT|      07|  1665000|\n",
      "| PAYMENT_REVIEW|      08|    68750|\n",
      "+---------------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT order_status, \n",
    "       date_format(to_date(substring(order_date, 1, 10), 'yyyy-MM-dd'), 'MM') AS MonthNum, \n",
    "       count(order_id) AS NoOfOrder\n",
    "FROM order \n",
    "GROUP BY order_status, date_format(to_date(substring(order_date, 1, 10), 'yyyy-MM-dd'), 'MM')\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a897925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['order_status, 'date_format('to_date('substring('order_date, 1, 10), yyyy-MM-dd), MM)], ['order_status, 'date_format('to_date('substring('order_date, 1, 10), yyyy-MM-dd), MM) AS MonthNum#93, 'count('order_id) AS NoOfOrder#94]\n",
      "+- 'UnresolvedRelation [order], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "order_status: string, MonthNum: string, NoOfOrder: bigint\n",
      "Aggregate [order_status#3, date_format(cast(to_date(substring(order_date#1, 1, 10), Some(yyyy-MM-dd)) as timestamp), MM, Some(America/Toronto))], [order_status#3, date_format(cast(to_date(substring(order_date#1, 1, 10), Some(yyyy-MM-dd)) as timestamp), MM, Some(America/Toronto)) AS MonthNum#93, count(order_id#0L) AS NoOfOrder#94L]\n",
      "+- SubqueryAlias order\n",
      "   +- Relation[order_id#0L,order_date#1,customer_id#2L,order_status#3] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [order_status#3, date_format(cast(cast(gettimestamp(substring(order_date#1, 1, 10), yyyy-MM-dd, Some(America/Toronto), false) as date) as timestamp), MM, Some(America/Toronto))], [order_status#3, date_format(cast(cast(gettimestamp(substring(order_date#1, 1, 10), yyyy-MM-dd, Some(America/Toronto), false) as date) as timestamp), MM, Some(America/Toronto)) AS MonthNum#93, count(order_id#0L) AS NoOfOrder#94L]\n",
      "+- Project [order_id#0L, order_date#1, order_status#3]\n",
      "   +- Relation[order_id#0L,order_date#1,customer_id#2L,order_status#3] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[order_status#3, date_format(cast(cast(gettimestamp(substring(order_date#1, 1, 10), yyyy-MM-dd, Some(America/Toronto), false) as date) as timestamp), MM, Some(America/Toronto))#99], functions=[count(order_id#0L)], output=[order_status#3, MonthNum#93, NoOfOrder#94L])\n",
      "+- Exchange hashpartitioning(order_status#3, date_format(cast(cast(gettimestamp(substring(order_date#1, 1, 10), yyyy-MM-dd, Some(America/Toronto), false) as date) as timestamp), MM, Some(America/Toronto))#99, 200), ENSURE_REQUIREMENTS, [id=#78]\n",
      "   +- *(1) HashAggregate(keys=[order_status#3, date_format(cast(cast(gettimestamp(substring(order_date#1, 1, 10), yyyy-MM-dd, Some(America/Toronto), false) as date) as timestamp), MM, Some(America/Toronto)) AS date_format(cast(cast(gettimestamp(substring(order_date#1, 1, 10), yyyy-MM-dd, Some(America/Toronto), false) as date) as timestamp), MM, Some(America/Toronto))#99], functions=[partial_count(order_id#0L)], output=[order_status#3, date_format(cast(cast(gettimestamp(substring(order_date#1, 1, 10), yyyy-MM-dd, Some(America/Toronto), false) as date) as timestamp), MM, Some(America/Toronto))#99, count#101L])\n",
      "      +- FileScan csv [order_id#0L,order_date#1,order_status#3] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://m01.itversity.com:9000/user/itv017244/week11Assignment/orders.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_id:bigint,order_date:string,order_status:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT order_status, \n",
    "       date_format(to_date(substring(order_date, 1, 10), 'yyyy-MM-dd'), 'MM') AS MonthNum, \n",
    "       count(order_id) AS NoOfOrder\n",
    "FROM order \n",
    "GROUP BY order_status, date_format(to_date(substring(order_date, 1, 10), 'yyyy-MM-dd'), 'MM')\n",
    "\"\"\").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "96c40148",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_schema = 'order_id long, order_date date , customer_id long, order_status string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1c92e78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df = spark.read.\\\n",
    "format(\"csv\").\\\n",
    "schema(order_schema).\\\n",
    "load(\"/user/itv017244/week11Assignment/orders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0cba9eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df.createOrReplaceTempView(\"order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8492dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+---------+\n",
      "|   order_status|MonthNum|NoOfOrder|\n",
      "+---------------+--------+---------+\n",
      "|        ON_HOLD|      05|   386250|\n",
      "|       COMPLETE|      07|  2417500|\n",
      "|         CLOSED|      09|   845000|\n",
      "|         CLOSED|      01|   791250|\n",
      "|SUSPECTED_FRAUD|      09|   185000|\n",
      "|       CANCELED|      02|   156250|\n",
      "|SUSPECTED_FRAUD|      06|   163750|\n",
      "|        PENDING|      07|   847500|\n",
      "|       COMPLETE|      08|  2350000|\n",
      "|       CANCELED|      08|   156250|\n",
      "|SUSPECTED_FRAUD|      10|   135000|\n",
      "|        ON_HOLD|      04|   416250|\n",
      "| PAYMENT_REVIEW|      07|    91250|\n",
      "|SUSPECTED_FRAUD|      07|   167500|\n",
      "|SUSPECTED_FRAUD|      01|   163750|\n",
      "|        ON_HOLD|      03|   403750|\n",
      "|       CANCELED|      06|   130000|\n",
      "|       COMPLETE|      06|  2246250|\n",
      "|PENDING_PAYMENT|      07|  1665000|\n",
      "| PAYMENT_REVIEW|      08|    68750|\n",
      "+---------------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT order_status, \n",
    "       date_format(order_date,'MM') AS MonthNum, \n",
    "       count(order_id) AS NoOfOrder\n",
    "FROM order \n",
    "GROUP BY order_status, date_format(order_date, 'MM')\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "78a33795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['order_status, 'date_format('order_date, MM)], ['order_status, 'date_format('order_date, MM) AS MonthNum#169, 'count('order_id) AS NoOfOrder#170]\n",
      "+- 'UnresolvedRelation [order], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "order_status: string, MonthNum: string, NoOfOrder: bigint\n",
      "Aggregate [order_status#139, date_format(cast(order_date#137 as timestamp), MM, Some(America/Toronto))], [order_status#139, date_format(cast(order_date#137 as timestamp), MM, Some(America/Toronto)) AS MonthNum#169, count(order_id#136L) AS NoOfOrder#170L]\n",
      "+- SubqueryAlias order\n",
      "   +- Relation[order_id#136L,order_date#137,customer_id#138L,order_status#139] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [order_status#139, date_format(cast(order_date#137 as timestamp), MM, Some(America/Toronto))], [order_status#139, date_format(cast(order_date#137 as timestamp), MM, Some(America/Toronto)) AS MonthNum#169, count(order_id#136L) AS NoOfOrder#170L]\n",
      "+- Project [order_id#136L, order_date#137, order_status#139]\n",
      "   +- Relation[order_id#136L,order_date#137,customer_id#138L,order_status#139] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[order_status#139, date_format(cast(order_date#137 as timestamp), MM, Some(America/Toronto))#175], functions=[count(order_id#136L)], output=[order_status#139, MonthNum#169, NoOfOrder#170L])\n",
      "+- Exchange hashpartitioning(order_status#139, date_format(cast(order_date#137 as timestamp), MM, Some(America/Toronto))#175, 200), ENSURE_REQUIREMENTS, [id=#151]\n",
      "   +- *(1) HashAggregate(keys=[order_status#139, date_format(cast(order_date#137 as timestamp), MM, Some(America/Toronto)) AS date_format(cast(order_date#137 as timestamp), MM, Some(America/Toronto))#175], functions=[partial_count(order_id#136L)], output=[order_status#139, date_format(cast(order_date#137 as timestamp), MM, Some(America/Toronto))#175, count#177L])\n",
      "      +- FileScan csv [order_id#136L,order_date#137,order_status#139] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://m01.itversity.com:9000/user/itv017244/week11Assignment/orders.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_id:bigint,order_date:date,order_status:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT order_status, \n",
    "       date_format(order_date,'MM') AS MonthNum, \n",
    "       count(order_id) AS NoOfOrder\n",
    "FROM order \n",
    "GROUP BY order_status, date_format(order_date, 'MM')\n",
    "\"\"\").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8137fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT order_status, \n",
    "       first(int(date_format(order_date,'MM'))) AS MonthNum, \n",
    "       count(order_id) AS NoOfOrder\n",
    "FROM order \n",
    "GROUP BY order_status order by  MonthNum\n",
    "\"\"\").write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "31f842f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_schema = 'order_id long, order_date string , customer_id long, order_status string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5ec06c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df = spark.read.\\\n",
    "format(\"csv\").\\\n",
    "schema(order_schema).\\\n",
    "load(\"/user/itv017244/week11Assignment/orders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7b220953",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df.createOrReplaceTempView(\"order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7ec5f427",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT order_status, \n",
    "       first(date_format(order_date,'MM')) AS MonthNum, \n",
    "       count(order_id) AS NoOfOrder\n",
    "FROM order \n",
    "GROUP BY order_status order by  MonthNum\n",
    "\"\"\").write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6fa185d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project ['order_status, 'customer_id]\n",
      "+- 'Filter ('order_status = OPEN)\n",
      "   +- 'SubqueryAlias __auto_generated_subquery_name\n",
      "      +- 'Project ['order_status, 'customer_id]\n",
      "         +- 'Filter ('order_status = COMPLETED)\n",
      "            +- 'UnresolvedRelation [order], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "order_status: string, customer_id: bigint\n",
      "Project [order_status#179, customer_id#178L]\n",
      "+- Filter (order_status#179 = OPEN)\n",
      "   +- SubqueryAlias __auto_generated_subquery_name\n",
      "      +- Project [order_status#179, customer_id#178L]\n",
      "         +- Filter (order_status#179 = COMPLETED)\n",
      "            +- SubqueryAlias order\n",
      "               +- Relation[order_id#176L,order_date#177,customer_id#178L,order_status#179] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [order_status#179, customer_id#178L]\n",
      "+- Filter ((isnotnull(order_status#179) AND (order_status#179 = COMPLETED)) AND (order_status#179 = OPEN))\n",
      "   +- Relation[order_id#176L,order_date#177,customer_id#178L,order_status#179] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [order_status#179, customer_id#178L]\n",
      "+- *(1) Filter ((isnotnull(order_status#179) AND (order_status#179 = COMPLETED)) AND (order_status#179 = OPEN))\n",
      "   +- FileScan csv [customer_id#178L,order_status#179] Batched: false, DataFilters: [isnotnull(order_status#179), (order_status#179 = COMPLETED), (order_status#179 = OPEN)], Format: CSV, Location: InMemoryFileIndex[hdfs://m01.itversity.com:9000/user/itv017244/week11Assignment/orders.csv], PartitionFilters: [], PushedFilters: [IsNotNull(order_status), EqualTo(order_status,COMPLETED), EqualTo(order_status,OPEN)], ReadSchema: struct<customer_id:bigint,order_status:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT order_status, customer_id \n",
    "FROM (select order_status, customer_id  from order where order_status =='COMPLETED')\n",
    "where order_status = 'OPEN'\n",
    "\"\"\").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1550ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_schema = 'order_id long, order_date date , customer_id long' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3e1aa0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df = spark.read.\\\n",
    "format(\"csv\").\\\n",
    "schema(order_schema).\\\n",
    "load(\"/public/trendytech/datasets/parquet-schema-evol-demo/csv/orders2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fb1bd07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df.write.mode(\"append\").option(\"mergeSchema\",True).option(\"path\",\"/user/itv017244/week11Assignment/\").saveAsTable(\"Week11_assignment1_order1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2ec47215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+\n",
      "|order_id|order_date|customer_id|\n",
      "+--------+----------+-----------+\n",
      "|       3|2013-07-25|      12111|\n",
      "|       4|2013-07-25|       8827|\n",
      "+--------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from Week11_assignment1_order1\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "24efc480",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_schema = 'order_id long, order_date date , customer_id long, order_status string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fdf3c507",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df = spark.read.\\\n",
    "format(\"csv\").\\\n",
    "schema(order_schema).\\\n",
    "load(\"/public/trendytech/datasets/parquet-schema-evol-demo/csv/orders3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "635372aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "The column number of the existing table default.Week11_assignment1_order1(struct<order_id:bigint,order_date:date,customer_id:bigint>) doesn't match the data schema(struct<order_id:bigint,order_date:date,order_status:string,customer_id:bigint>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-f6eb792193e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0morder_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"append\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mergeSchema\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/user/itv017244/week11Assignment/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Week11_assignment1_order1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msaveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m     def json(self, path, mode=None, compression=None, dateFormat=None, timestampFormat=None,\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: The column number of the existing table default.Week11_assignment1_order1(struct<order_id:bigint,order_date:date,customer_id:bigint>) doesn't match the data schema(struct<order_id:bigint,order_date:date,order_status:string,customer_id:bigint>)"
     ]
    }
   ],
   "source": [
    "order_df.write.mode(\"append\").option(\"mergeSchema\",True).option(\"path\",\"/user/itv017244/week11Assignment/\").saveAsTable(\"Week11_assignment1_order1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6be9ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_schema = 'order_id long, order_date date '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2040e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df = spark.read.\\\n",
    "format(\"csv\").\\\n",
    "schema(order_schema).\\\n",
    "load(\"/public/trendytech/datasets/parquet-schema-evol-demo/csv/orders1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5b82dec7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "The column number of the existing table default.Week11_assignment1_order1(struct<order_id:bigint,order_date:date,customer_id:bigint>) doesn't match the data schema(struct<order_id:bigint,order_date:date>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-f6eb792193e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0morder_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"append\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mergeSchema\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/user/itv017244/week11Assignment/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Week11_assignment1_order1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msaveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m     def json(self, path, mode=None, compression=None, dateFormat=None, timestampFormat=None,\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: The column number of the existing table default.Week11_assignment1_order1(struct<order_id:bigint,order_date:date,customer_id:bigint>) doesn't match the data schema(struct<order_id:bigint,order_date:date>)"
     ]
    }
   ],
   "source": [
    "order_df.write.mode(\"append\").option(\"mergeSchema\",True).option(\"path\",\"/user/itv017244/week11Assignment/\").saveAsTable(\"Week11_assignment1_order1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "82a231a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df.write.mode(\"append\").option(\"path\",\"/user/itv017244/week11Assignment/order1\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9747b36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = spark.read.format(\"parquet\").option(\"mergeSchema\",True).load(\"/user/itv017244/week11Assignment/order1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "01fedfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|order_id|order_date|\n",
      "+--------+----------+\n",
      "|       1|2013-07-25|\n",
      "|       2|2013-07-25|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "390f123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_schema = 'order_id long, order_date date , customer_id long, order_status string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "85208fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df = spark.read.\\\n",
    "format(\"csv\").\\\n",
    "schema(order_schema).\\\n",
    "load(\"/public/trendytech/datasets/parquet-schema-evol-demo/csv/orders3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "96e4e511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+------------+\n",
      "|order_id|order_date|customer_id|order_status|\n",
      "+--------+----------+-----------+------------+\n",
      "|       5|2013-07-25|      11318|    COMPLETE|\n",
      "|       6|2013-07-25|       7130|    COMPLETE|\n",
      "+--------+----------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d6e3c767",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df.write.mode(\"append\").option(\"path\",\"/user/itv017244/week11Assignment/order1\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e8d3d1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = spark.read.option(\"mergeSchema\",True).load(\"/user/itv017244/week11Assignment/order1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "57512556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+------------+\n",
      "|order_id|order_date|customer_id|order_status|\n",
      "+--------+----------+-----------+------------+\n",
      "|       5|2013-07-25|      11318|    COMPLETE|\n",
      "|       6|2013-07-25|       7130|    COMPLETE|\n",
      "|       1|2013-07-25|       null|        null|\n",
      "|       2|2013-07-25|       null|        null|\n",
      "+--------+----------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8d9d4e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_schema = 'order_id long, order_date date , customer_id long' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e1258098",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df = spark.read.\\\n",
    "format(\"csv\").\\\n",
    "schema(order_schema).\\\n",
    "load(\"/public/trendytech/datasets/parquet-schema-evol-demo/csv/orders2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b6bfcea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+\n",
      "|order_id|order_date|customer_id|\n",
      "+--------+----------+-----------+\n",
      "|       3|2013-07-25|      12111|\n",
      "|       4|2013-07-25|       8827|\n",
      "+--------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "56fdee76",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df.write.mode(\"append\").option(\"path\",\"/user/itv017244/week11Assignment/order1\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "14dd3a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = spark.read.option(\"mergeSchema\",True).load(\"/user/itv017244/week11Assignment/order1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "29057e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+------------+\n",
      "|order_id|order_date|customer_id|order_status|\n",
      "+--------+----------+-----------+------------+\n",
      "|       5|2013-07-25|      11318|    COMPLETE|\n",
      "|       6|2013-07-25|       7130|    COMPLETE|\n",
      "|       3|2013-07-25|      12111|        null|\n",
      "|       4|2013-07-25|       8827|        null|\n",
      "|       1|2013-07-25|       null|        null|\n",
      "|       2|2013-07-25|       null|        null|\n",
      "+--------+----------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e3dcb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_schema = 'order_id long, order_date date, customer_id long, order_status string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0eae0caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df = spark.read.\\\n",
    "format(\"csv\").\\\n",
    "schema(order_schema).\\\n",
    "load(\"/public/trendytech/orders/orders_1gb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24267261",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df.coalesce(1). \\\n",
    "write. \\\n",
    "format(\"orc\"). \\\n",
    "option(\"compression\", \"lzo\"). \\\n",
    "mode(\"overwrite\"). \\\n",
    "option(\"path\", \"/user/itv017244/datasets/compression-techniques-demo/orc_lzo\"). \\\n",
    "save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c10a4e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df.coalesce(1). \\\n",
    "write. \\\n",
    "format(\"orc\"). \\\n",
    "option(\"compression\", \"snappy\"). \\\n",
    "mode(\"overwrite\"). \\\n",
    "option(\"path\", \"/user/itv017244/datasets/compression-techniques-demo/orc_snappy\"). \\\n",
    "save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdcb870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
