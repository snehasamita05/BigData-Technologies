{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f5349fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession. \\\n",
    "builder. \\\n",
    "appName(\"Sneha Spark Session\").\\\n",
    "config(\"spark.shuffle.useOldFetchProtocol\", 'true'). \\\n",
    "config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "enableHiveSupport(). \\\n",
    "master('yarn'). \\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a428e7d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://g01.itversity.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Sneha Spark Session</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9f094f7748>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2913bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_schema = 'order_id long, order_date date, customer_id long, order_status string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c3afe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df = spark.read.\\\n",
    "format(\"csv\").\\\n",
    "schema(order_schema).\\\n",
    "load(\"/public/trendytech/orders/orders_1gb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a37d18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+---------------+\n",
      "|order_id|order_date|customer_id|   order_status|\n",
      "+--------+----------+-----------+---------------+\n",
      "|       1|2013-07-25|      11599|         CLOSED|\n",
      "|       2|2013-07-25|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25|      12111|       COMPLETE|\n",
      "|       4|2013-07-25|       8827|         CLOSED|\n",
      "|       5|2013-07-25|      11318|       COMPLETE|\n",
      "|       6|2013-07-25|       7130|       COMPLETE|\n",
      "|       7|2013-07-25|       4530|       COMPLETE|\n",
      "|       8|2013-07-25|       2911|     PROCESSING|\n",
      "|       9|2013-07-25|       5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25|       5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25|        918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25|       1837|         CLOSED|\n",
      "|      13|2013-07-25|       9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25|       9842|     PROCESSING|\n",
      "|      15|2013-07-25|       2568|       COMPLETE|\n",
      "|      16|2013-07-25|       7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25|       2667|       COMPLETE|\n",
      "|      18|2013-07-25|       1205|         CLOSED|\n",
      "|      19|2013-07-25|       9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25|       9198|     PROCESSING|\n",
      "+--------+----------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f86b74eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df.createOrReplaceTempView(\"order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c63ba36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>order_id</th><th>order_date</th><th>customer_id</th><th>order_status</th></tr>\n",
       "<tr><td>1</td><td>2013-07-25</td><td>11599</td><td>CLOSED</td></tr>\n",
       "<tr><td>2</td><td>2013-07-25</td><td>256</td><td>PENDING_PAYMENT</td></tr>\n",
       "<tr><td>3</td><td>2013-07-25</td><td>12111</td><td>COMPLETE</td></tr>\n",
       "<tr><td>4</td><td>2013-07-25</td><td>8827</td><td>CLOSED</td></tr>\n",
       "<tr><td>5</td><td>2013-07-25</td><td>11318</td><td>COMPLETE</td></tr>\n",
       "<tr><td>6</td><td>2013-07-25</td><td>7130</td><td>COMPLETE</td></tr>\n",
       "<tr><td>7</td><td>2013-07-25</td><td>4530</td><td>COMPLETE</td></tr>\n",
       "<tr><td>8</td><td>2013-07-25</td><td>2911</td><td>PROCESSING</td></tr>\n",
       "<tr><td>9</td><td>2013-07-25</td><td>5657</td><td>PENDING_PAYMENT</td></tr>\n",
       "<tr><td>10</td><td>2013-07-25</td><td>5648</td><td>PENDING_PAYMENT</td></tr>\n",
       "<tr><td>11</td><td>2013-07-25</td><td>918</td><td>PAYMENT_REVIEW</td></tr>\n",
       "<tr><td>12</td><td>2013-07-25</td><td>1837</td><td>CLOSED</td></tr>\n",
       "<tr><td>13</td><td>2013-07-25</td><td>9149</td><td>PENDING_PAYMENT</td></tr>\n",
       "<tr><td>14</td><td>2013-07-25</td><td>9842</td><td>PROCESSING</td></tr>\n",
       "<tr><td>15</td><td>2013-07-25</td><td>2568</td><td>COMPLETE</td></tr>\n",
       "<tr><td>16</td><td>2013-07-25</td><td>7276</td><td>PENDING_PAYMENT</td></tr>\n",
       "<tr><td>17</td><td>2013-07-25</td><td>2667</td><td>COMPLETE</td></tr>\n",
       "<tr><td>18</td><td>2013-07-25</td><td>1205</td><td>CLOSED</td></tr>\n",
       "<tr><td>19</td><td>2013-07-25</td><td>9488</td><td>PENDING_PAYMENT</td></tr>\n",
       "<tr><td>20</td><td>2013-07-25</td><td>9198</td><td>PROCESSING</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------+----------+-----------+---------------+\n",
       "|order_id|order_date|customer_id|   order_status|\n",
       "+--------+----------+-----------+---------------+\n",
       "|       1|2013-07-25|      11599|         CLOSED|\n",
       "|       2|2013-07-25|        256|PENDING_PAYMENT|\n",
       "|       3|2013-07-25|      12111|       COMPLETE|\n",
       "|       4|2013-07-25|       8827|         CLOSED|\n",
       "|       5|2013-07-25|      11318|       COMPLETE|\n",
       "|       6|2013-07-25|       7130|       COMPLETE|\n",
       "|       7|2013-07-25|       4530|       COMPLETE|\n",
       "|       8|2013-07-25|       2911|     PROCESSING|\n",
       "|       9|2013-07-25|       5657|PENDING_PAYMENT|\n",
       "|      10|2013-07-25|       5648|PENDING_PAYMENT|\n",
       "|      11|2013-07-25|        918| PAYMENT_REVIEW|\n",
       "|      12|2013-07-25|       1837|         CLOSED|\n",
       "|      13|2013-07-25|       9149|PENDING_PAYMENT|\n",
       "|      14|2013-07-25|       9842|     PROCESSING|\n",
       "|      15|2013-07-25|       2568|       COMPLETE|\n",
       "|      16|2013-07-25|       7276|PENDING_PAYMENT|\n",
       "|      17|2013-07-25|       2667|       COMPLETE|\n",
       "|      18|2013-07-25|       1205|         CLOSED|\n",
       "|      19|2013-07-25|       9488|PENDING_PAYMENT|\n",
       "|      20|2013-07-25|       9198|     PROCESSING|\n",
       "+--------+----------+-----------+---------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaf7c444",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\nextraneous input 'order' expecting {<EOF>, ';'}(line 1, pos 13)\n\n== SQL ==\nselect * fra order\n-------------^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c7ef818bfe39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select * fra order\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \"\"\"\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParseException\u001b[0m: \nextraneous input 'order' expecting {<EOF>, ';'}(line 1, pos 13)\n\n== SQL ==\nselect * fra order\n-------------^^^\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * fra order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c3b90ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table or view not found: orders; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [orders], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a250d888fd14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select * from orders\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \"\"\"\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table or view not found: orders; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [orders], [], false\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c7a1fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|order_id|   order_status|\n",
      "+--------+---------------+\n",
      "|       1|         CLOSED|\n",
      "|       2|PENDING_PAYMENT|\n",
      "|       3|       COMPLETE|\n",
      "|       4|         CLOSED|\n",
      "|       5|       COMPLETE|\n",
      "|       6|       COMPLETE|\n",
      "|       7|       COMPLETE|\n",
      "|       8|     PROCESSING|\n",
      "|       9|PENDING_PAYMENT|\n",
      "|      10|PENDING_PAYMENT|\n",
      "|      11| PAYMENT_REVIEW|\n",
      "|      12|         CLOSED|\n",
      "|      13|PENDING_PAYMENT|\n",
      "|      14|     PROCESSING|\n",
      "|      15|       COMPLETE|\n",
      "|      16|PENDING_PAYMENT|\n",
      "|      17|       COMPLETE|\n",
      "|      18|         CLOSED|\n",
      "|      19|PENDING_PAYMENT|\n",
      "|      20|     PROCESSING|\n",
      "+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select order_id,order_status from (select order_id,customer_id\n",
    ",order_status from order where order_id<500) where order_id<200\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "882e14c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project ['order_id, 'order_status]\n",
      "+- 'Filter ('order_id < 200)\n",
      "   +- 'SubqueryAlias __auto_generated_subquery_name\n",
      "      +- 'Project ['order_id, 'customer_id, 'order_status]\n",
      "         +- 'Filter ('order_id < 500)\n",
      "            +- 'UnresolvedRelation [order], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "order_id: bigint, order_status: string\n",
      "Project [order_id#0L, order_status#3]\n",
      "+- Filter (order_id#0L < cast(200 as bigint))\n",
      "   +- SubqueryAlias __auto_generated_subquery_name\n",
      "      +- Project [order_id#0L, customer_id#2L, order_status#3]\n",
      "         +- Filter (order_id#0L < cast(500 as bigint))\n",
      "            +- SubqueryAlias order\n",
      "               +- Relation[order_id#0L,order_date#1,customer_id#2L,order_status#3] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [order_id#0L, order_status#3]\n",
      "+- Filter ((isnotnull(order_id#0L) AND (order_id#0L < 500)) AND (order_id#0L < 200))\n",
      "   +- Relation[order_id#0L,order_date#1,customer_id#2L,order_status#3] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter ((isnotnull(order_id#0L) AND (order_id#0L < 500)) AND (order_id#0L < 200))\n",
      "+- FileScan csv [order_id#0L,order_status#3] Batched: false, DataFilters: [isnotnull(order_id#0L), (order_id#0L < 500), (order_id#0L < 200)], Format: CSV, Location: InMemoryFileIndex[hdfs://m01.itversity.com:9000/public/trendytech/orders/orders_1gb.csv], PartitionFilters: [], PushedFilters: [IsNotNull(order_id), LessThan(order_id,500), LessThan(order_id,200)], ReadSchema: struct<order_id:bigint,order_status:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select order_id,order_status from (select order_id,customer_id\n",
    ",order_status from order where order_id<500) where order_id<200\n",
    "\"\"\").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbd91142",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_schema = \"customer_id long,customer_fname string, customer_lname string,username string, password string,address string, city string, state string, pincode long\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47d46dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = spark.read.format(\"csv\").schema(customer_schema).load(\"/public/trendytech/retail_db/customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d3e0161",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df.createOrReplaceTempView(\"customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9399e11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+------------+-----------+--------------+--------------+---------+---------+--------------------+------------+-----+-------+\n",
      "|order_id|order_date|customer_id|order_status|customer_id|customer_fname|customer_lname| username| password|             address|        city|state|pincode|\n",
      "+--------+----------+-----------+------------+-----------+--------------+--------------+---------+---------+--------------------+------------+-----+-------+\n",
      "|       1|2013-07-25|      11599|      CLOSED|      11599|          Mary|        Malone|XXXXXXXXX|XXXXXXXXX|8708 Indian Horse...|     Hickory|   NC|  28601|\n",
      "|       4|2013-07-25|       8827|      CLOSED|       8827|         Brian|        Wilson|XXXXXXXXX|XXXXXXXXX|   8396 High Corners| San Antonio|   TX|  78240|\n",
      "|      12|2013-07-25|       1837|      CLOSED|       1837|          Mary|          Vega|XXXXXXXXX|XXXXXXXXX|  4312 Bright Corner|      Caguas|   PR|    725|\n",
      "|      18|2013-07-25|       1205|      CLOSED|       1205|          Mary|        Powell|XXXXXXXXX|XXXXXXXXX|9299 Quiet Pionee...|       Miami|   FL|  33126|\n",
      "|      24|2013-07-25|      11441|      CLOSED|      11441|          Mary|      Ferguson|XXXXXXXXX|XXXXXXXXX|8702 Umber Mounta...|        Lutz|   FL|  33549|\n",
      "|      25|2013-07-25|       9503|      CLOSED|       9503|          Mary|   Fitzpatrick|XXXXXXXXX|XXXXXXXXX|    7861 Honey Acres|     Orlando|   FL|  32822|\n",
      "|      37|2013-07-25|       5863|      CLOSED|       5863|           Amy|         Smith|XXXXXXXXX|XXXXXXXXX|8187 Cotton Rise ...|     Cordova|   TN|  38018|\n",
      "|      51|2013-07-25|      12271|      CLOSED|      12271|          Mary|         Small|XXXXXXXXX|XXXXXXXXX|6599 Grand Island...|     Ontario|   CA|  91761|\n",
      "|      57|2013-07-25|       7073|      CLOSED|       7073|          Joan|         Smith|XXXXXXXXX|XXXXXXXXX|1987 Grand Conces...|     Del Rio|   TX|  78840|\n",
      "|      61|2013-07-25|       4791|      CLOSED|       4791|          Mary|        Patton|XXXXXXXXX|XXXXXXXXX|    4696 Shady Mount|     Hialeah|   FL|  33010|\n",
      "|      62|2013-07-25|       9111|      CLOSED|       9111|          Mary|         Smith|XXXXXXXXX|XXXXXXXXX|  2122 Green By-pass|      Caguas|   PR|    725|\n",
      "|      87|2013-07-25|       3065|      CLOSED|       3065|           Amy|      Cummings|XXXXXXXXX|XXXXXXXXX|  9658 Merry Landing|      Caguas|   PR|    725|\n",
      "|      90|2013-07-25|       9131|      CLOSED|       9131|          Mary|         Smith|XXXXXXXXX|XXXXXXXXX|3871 Umber Blosso...|  New Castle|   DE|  19720|\n",
      "|     101|2013-07-25|       5116|      CLOSED|       5116|         Doris|         Smith|XXXXXXXXX|XXXXXXXXX|     1819 Noble Ramp|    Brockton|   MA|   2301|\n",
      "|     116|2013-07-26|       8763|      CLOSED|       8763|        Pamela|      Williams|XXXXXXXXX|XXXXXXXXX|5567 Cotton Timbe...|      Santee|   CA|  92071|\n",
      "|     129|2013-07-26|       9937|      CLOSED|       9937|          Mary|        Atkins|XXXXXXXXX|XXXXXXXXX|8766 Easy Leaf Knoll| West Covina|   CA|  91790|\n",
      "|     133|2013-07-26|      10604|      CLOSED|      10604|      Jennifer|         Smith|XXXXXXXXX|XXXXXXXXX|   2905 Sleepy Route|Fayetteville|   NC|  28314|\n",
      "|     191|2013-07-26|         16|      CLOSED|         16|       Tiffany|         Smith|XXXXXXXXX|XXXXXXXXX|      6651 Iron Port|      Caguas|   PR|    725|\n",
      "|     201|2013-07-26|       9055|      CLOSED|       9055|         Karen|          Ross|XXXXXXXXX|XXXXXXXXX|5207 Dewy Butterf...|      Caguas|   PR|    725|\n",
      "|     211|2013-07-26|      10372|      CLOSED|      10372|         Amber|        Martin|XXXXXXXXX|XXXXXXXXX|  6481 Golden Island|      Caguas|   PR|    725|\n",
      "+--------+----------+-----------+------------+-----------+--------------+--------------+---------+---------+--------------------+------------+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select * from order join customer on order.customer_id == customer.customer_id\n",
    "where order_status = 'CLOSED'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70d68bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [*]\n",
      "+- 'Filter ('order_status = CLOSED)\n",
      "   +- 'Join Inner, ('order.customer_id = 'customer.customer_id)\n",
      "      :- 'UnresolvedRelation [order], [], false\n",
      "      +- 'UnresolvedRelation [customer], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "order_id: bigint, order_date: date, customer_id: bigint, order_status: string, customer_id: bigint, customer_fname: string, customer_lname: string, username: string, password: string, address: string, city: string, state: string, pincode: bigint\n",
      "Project [order_id#0L, order_date#1, customer_id#2L, order_status#3, customer_id#90L, customer_fname#91, customer_lname#92, username#93, password#94, address#95, city#96, state#97, pincode#98L]\n",
      "+- Filter (order_status#3 = CLOSED)\n",
      "   +- Join Inner, (customer_id#2L = customer_id#90L)\n",
      "      :- SubqueryAlias order\n",
      "      :  +- Relation[order_id#0L,order_date#1,customer_id#2L,order_status#3] csv\n",
      "      +- SubqueryAlias customer\n",
      "         +- Relation[customer_id#90L,customer_fname#91,customer_lname#92,username#93,password#94,address#95,city#96,state#97,pincode#98L] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join Inner, (customer_id#2L = customer_id#90L)\n",
      ":- Filter ((isnotnull(order_status#3) AND (order_status#3 = CLOSED)) AND isnotnull(customer_id#2L))\n",
      ":  +- Relation[order_id#0L,order_date#1,customer_id#2L,order_status#3] csv\n",
      "+- Filter isnotnull(customer_id#90L)\n",
      "   +- Relation[customer_id#90L,customer_fname#91,customer_lname#92,username#93,password#94,address#95,city#96,state#97,pincode#98L] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [customer_id#2L], [customer_id#90L], Inner, BuildRight, false\n",
      ":- *(2) Filter ((isnotnull(order_status#3) AND (order_status#3 = CLOSED)) AND isnotnull(customer_id#2L))\n",
      ":  +- FileScan csv [order_id#0L,order_date#1,customer_id#2L,order_status#3] Batched: false, DataFilters: [isnotnull(order_status#3), (order_status#3 = CLOSED), isnotnull(customer_id#2L)], Format: CSV, Location: InMemoryFileIndex[hdfs://m01.itversity.com:9000/public/trendytech/orders/orders_1gb.csv], PartitionFilters: [], PushedFilters: [IsNotNull(order_status), EqualTo(order_status,CLOSED), IsNotNull(customer_id)], ReadSchema: struct<order_id:bigint,order_date:date,customer_id:bigint,order_status:string>\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [id=#138]\n",
      "   +- *(1) Filter isnotnull(customer_id#90L)\n",
      "      +- FileScan csv [customer_id#90L,customer_fname#91,customer_lname#92,username#93,password#94,address#95,city#96,state#97,pincode#98L] Batched: false, DataFilters: [isnotnull(customer_id#90L)], Format: CSV, Location: InMemoryFileIndex[hdfs://m01.itversity.com:9000/public/trendytech/retail_db/customers], PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<customer_id:bigint,customer_fname:string,customer_lname:string,username:string,password:st...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select * from order join customer on order.customer_id == customer.customer_id\n",
    "where order_status = 'CLOSED'\n",
    "\"\"\").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d1e90a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|customer_id|count(1)|\n",
      "+-----------+--------+\n",
      "|          1|     375|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select customer_id ,count(1) from (select * from order\n",
    " where customer_id in (1,2,3,4,5)) where customer_id in (1,2,3) group by customer_id\n",
    " having customer_id = 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d54290b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'UnresolvedHaving ('customer_id = 1)\n",
      "+- 'Aggregate ['customer_id], ['customer_id, unresolvedalias('count(1), None)]\n",
      "   +- 'Filter 'customer_id IN (1,2,3)\n",
      "      +- 'SubqueryAlias __auto_generated_subquery_name\n",
      "         +- 'Project [*]\n",
      "            +- 'Filter 'customer_id IN (1,2,3,4,5)\n",
      "               +- 'UnresolvedRelation [order], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "customer_id: bigint, count(1): bigint\n",
      "Filter (customer_id#2L = cast(1 as bigint))\n",
      "+- Aggregate [customer_id#2L], [customer_id#2L, count(1) AS count(1)#251L]\n",
      "   +- Filter cast(customer_id#2L as bigint) IN (cast(1 as bigint),cast(2 as bigint),cast(3 as bigint))\n",
      "      +- SubqueryAlias __auto_generated_subquery_name\n",
      "         +- Project [order_id#0L, order_date#1, customer_id#2L, order_status#3]\n",
      "            +- Filter cast(customer_id#2L as bigint) IN (cast(1 as bigint),cast(2 as bigint),cast(3 as bigint),cast(4 as bigint),cast(5 as bigint))\n",
      "               +- SubqueryAlias order\n",
      "                  +- Relation[order_id#0L,order_date#1,customer_id#2L,order_status#3] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [customer_id#2L], [customer_id#2L, count(1) AS count(1)#251L]\n",
      "+- Project [customer_id#2L]\n",
      "   +- Filter (((isnotnull(customer_id#2L) AND customer_id#2L IN (1,2,3,4,5)) AND customer_id#2L IN (1,2,3)) AND (customer_id#2L = 1))\n",
      "      +- Relation[order_id#0L,order_date#1,customer_id#2L,order_status#3] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[customer_id#2L], functions=[count(1)], output=[customer_id#2L, count(1)#251L])\n",
      "+- Exchange hashpartitioning(customer_id#2L, 200), ENSURE_REQUIREMENTS, [id=#260]\n",
      "   +- *(1) HashAggregate(keys=[customer_id#2L], functions=[partial_count(1)], output=[customer_id#2L, count#259L])\n",
      "      +- *(1) Filter (((isnotnull(customer_id#2L) AND customer_id#2L IN (1,2,3,4,5)) AND customer_id#2L IN (1,2,3)) AND (customer_id#2L = 1))\n",
      "         +- FileScan csv [customer_id#2L] Batched: false, DataFilters: [isnotnull(customer_id#2L), customer_id#2L IN (1,2,3,4,5), customer_id#2L IN (1,2,3), (customer_i..., Format: CSV, Location: InMemoryFileIndex[hdfs://m01.itversity.com:9000/public/trendytech/orders/orders_1gb.csv], PartitionFilters: [], PushedFilters: [IsNotNull(customer_id), In(customer_id, [1,2,3,4,5]), In(customer_id, [1,2,3]), EqualTo(customer..., ReadSchema: struct<customer_id:bigint>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select customer_id ,count(1) from (select * from order\n",
    " where customer_id in (1,2,3,4,5)) where customer_id in (1,2,3) group by customer_id\n",
    " having customer_id = 1\n",
    "\"\"\").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa13f35a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
