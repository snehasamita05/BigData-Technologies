{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4072901b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1207, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1033, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1212, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o46.sessionState",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-2d50f34e747b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.sql.warehouse.dir\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"/user/{username}/warehouse\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0menableHiveSupport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yarn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m                     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msessionState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetConfString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    334\u001b[0m             raise Py4JError(\n\u001b[1;32m    335\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 format(target_id, \".\", name))\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o46.sessionState"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession. \\\n",
    "builder. \\\n",
    "appName(\"Sneha Spark Session\").\\\n",
    "config(\"spark.shuffle.useOldFetchProtocol\", 'true'). \\\n",
    "config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "enableHiveSupport(). \\\n",
    "master('yarn'). \\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cd5955b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://g01.itversity.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Sneha Spark Session</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff73351b7b8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e228799",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_schema = 'order_id long, order_date date, customer_id long, order_status string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "130487de",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df = spark.read.\\\n",
    "format(\"csv\").\\\n",
    "schema(order_schema).\\\n",
    "load(\"/public/trendytech/retail_db/ordersnew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ccac289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+------------+\n",
      "|order_id|order_date|customer_id|order_status|\n",
      "+--------+----------+-----------+------------+\n",
      "|    2480|2013-08-07|       3807|    COMPLETE|\n",
      "|   30479|2014-01-30|       9265|    COMPLETE|\n",
      "|    2481|2013-08-07|       2476|    COMPLETE|\n",
      "|   30481|2014-01-30|       9240|    COMPLETE|\n",
      "|    2483|2013-08-07|      10453|    COMPLETE|\n",
      "|   30484|2014-01-30|       2876|    COMPLETE|\n",
      "|    2484|2013-08-07|       9256|    COMPLETE|\n",
      "|   30485|2014-01-30|       1069|    COMPLETE|\n",
      "|    2488|2013-08-07|       1255|    COMPLETE|\n",
      "|   30486|2014-01-30|       1151|    COMPLETE|\n",
      "|    2491|2013-08-07|        247|    COMPLETE|\n",
      "|   30487|2014-01-30|       6772|    COMPLETE|\n",
      "|    2495|2013-08-07|       9011|    COMPLETE|\n",
      "|   30489|2014-01-30|       5717|    COMPLETE|\n",
      "|    2498|2013-08-07|       1966|    COMPLETE|\n",
      "|   30490|2014-01-30|      12189|    COMPLETE|\n",
      "|    2511|2013-08-07|       8544|    COMPLETE|\n",
      "|   30492|2014-01-30|       3710|    COMPLETE|\n",
      "|    2515|2013-08-07|      12056|    COMPLETE|\n",
      "|   30494|2014-01-30|       2724|    COMPLETE|\n",
      "+--------+----------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd3cd872",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df.createOrReplaceTempView(\"order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bff5927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+------------+\n",
      "|order_id|order_date|customer_id|order_status|\n",
      "+--------+----------+-----------+------------+\n",
      "|    2480|2013-08-07|       3807|    COMPLETE|\n",
      "|   30479|2014-01-30|       9265|    COMPLETE|\n",
      "|    2481|2013-08-07|       2476|    COMPLETE|\n",
      "|   30481|2014-01-30|       9240|    COMPLETE|\n",
      "|    2483|2013-08-07|      10453|    COMPLETE|\n",
      "|   30484|2014-01-30|       2876|    COMPLETE|\n",
      "|    2484|2013-08-07|       9256|    COMPLETE|\n",
      "|   30485|2014-01-30|       1069|    COMPLETE|\n",
      "|    2488|2013-08-07|       1255|    COMPLETE|\n",
      "|   30486|2014-01-30|       1151|    COMPLETE|\n",
      "|    2491|2013-08-07|        247|    COMPLETE|\n",
      "|   30487|2014-01-30|       6772|    COMPLETE|\n",
      "|    2495|2013-08-07|       9011|    COMPLETE|\n",
      "|   30489|2014-01-30|       5717|    COMPLETE|\n",
      "|    2498|2013-08-07|       1966|    COMPLETE|\n",
      "|   30490|2014-01-30|      12189|    COMPLETE|\n",
      "|    2511|2013-08-07|       8544|    COMPLETE|\n",
      "|   30492|2014-01-30|       3710|    COMPLETE|\n",
      "|    2515|2013-08-07|      12056|    COMPLETE|\n",
      "|   30494|2014-01-30|       2724|    COMPLETE|\n",
      "+--------+----------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from order\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7a2fc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---------+---------+\n",
      "|customer_id|order_month|total_cnt|month_num|\n",
      "+-----------+-----------+---------+---------+\n",
      "|       1852|    January|      375|       01|\n",
      "|        332|    January|     2010|       01|\n",
      "|         18|    January|     4020|       01|\n",
      "|       4249|    January|     2760|       01|\n",
      "|        255|    January|     2010|       01|\n",
      "|       1182|    January|      375|       01|\n",
      "|       1606|    January|     2010|       01|\n",
      "|       2787|    January|     2010|       01|\n",
      "|       4318|    January|     2385|       01|\n",
      "|       4618|    January|     4770|       01|\n",
      "|       9905|    January|     2010|       01|\n",
      "|      10148|    January|     2010|       01|\n",
      "|      10209|    January|      375|       01|\n",
      "|      10635|    January|     2010|       01|\n",
      "|      11156|    January|      375|       01|\n",
      "|      11347|    January|     2010|       01|\n",
      "|       4409|    January|     2010|       01|\n",
      "|       7268|    January|     2010|       01|\n",
      "|       4353|    January|     2010|       01|\n",
      "|       7955|    January|      375|       01|\n",
      "+-----------+-----------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            select customer_id,date_format(order_date,'MMMM') as order_month,\n",
    "            count(*) as total_cnt,first(date_format(order_date,'MM')) as month_num\n",
    "            from order\n",
    "            group by customer_id,order_month\n",
    "            order by month_num      \n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db0dcb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "            select customer_id,date_format(order_date,'MMMM') as order_month,\n",
    "            count(*) as total_cnt,first(date_format(order_date,'MM')) as month_num\n",
    "            from order\n",
    "            group by customer_id,order_month\n",
    "            order by month_num      \n",
    "          \"\"\").write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d1486b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---------+---------+\n",
      "|customer_id|order_month|total_cnt|month_num|\n",
      "+-----------+-----------+---------+---------+\n",
      "|       4353|    January|     2010|        1|\n",
      "|       6742|    January|     2010|        1|\n",
      "|       5073|    January|     2385|        1|\n",
      "|       1850|    January|     2010|        1|\n",
      "|       5009|    January|     2010|        1|\n",
      "|       7326|    January|     2010|        1|\n",
      "|       3650|    January|     2010|        1|\n",
      "|       2502|    January|      750|        1|\n",
      "|      12424|    January|      375|        1|\n",
      "|       6524|    January|     2010|        1|\n",
      "|      10137|    January|      375|        1|\n",
      "|        616|    January|      375|        1|\n",
      "|      12164|    January|      375|        1|\n",
      "|       8254|    January|      375|        1|\n",
      "|       5452|    January|      375|        1|\n",
      "|       3047|    January|      375|        1|\n",
      "|       8698|    January|      750|        1|\n",
      "|       9583|    January|      375|        1|\n",
      "|       2508|    January|      375|        1|\n",
      "|      10023|    January|      375|        1|\n",
      "+-----------+-----------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            select customer_id,date_format(order_date,'MMMM') as order_month,\n",
    "            count(*) as total_cnt,first(int(date_format(order_date,'MM'))) as month_num\n",
    "            from order\n",
    "            group by customer_id,order_month\n",
    "            order by month_num      \n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96a0366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "            select customer_id,date_format(order_date,'MMMM') as order_month,\n",
    "            count(*) as total_cnt,first(int(date_format(order_date,'MM'))) as month_num\n",
    "            from order\n",
    "            group by customer_id,order_month\n",
    "            order by month_num      \n",
    "          \"\"\").write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c2577ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "            select customer_id,date_format(order_date,'MMMM') as order_month,\n",
    "            count(*) as total_cnt,first(date_format(order_date,'MM')) as month_num\n",
    "            from order\n",
    "            group by customer_id,order_month\n",
    "            order by month_num      \n",
    "          \"\"\").write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f35f3077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['month_num ASC NULLS FIRST], true\n",
      "+- 'Aggregate ['customer_id, 'order_month], ['customer_id, 'date_format('order_date, MMMM) AS order_month#325, 'count(1) AS total_cnt#326, first('int('date_format('order_date, MM)), false) AS month_num#328]\n",
      "   +- 'UnresolvedRelation [order], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "customer_id: bigint, order_month: string, total_cnt: bigint, month_num: int\n",
      "Sort [month_num#328 ASC NULLS FIRST], true\n",
      "+- Aggregate [customer_id#2L, date_format(cast(order_date#1 as timestamp), MMMM, Some(America/Toronto))], [customer_id#2L, date_format(cast(order_date#1 as timestamp), MMMM, Some(America/Toronto)) AS order_month#325, count(1) AS total_cnt#326L, first(cast(date_format(cast(order_date#1 as timestamp), MM, Some(America/Toronto)) as int), false) AS month_num#328]\n",
      "   +- SubqueryAlias order\n",
      "      +- Relation[order_id#0L,order_date#1,customer_id#2L,order_status#3] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [month_num#328 ASC NULLS FIRST], true\n",
      "+- Aggregate [customer_id#2L, date_format(cast(order_date#1 as timestamp), MMMM, Some(America/Toronto))], [customer_id#2L, date_format(cast(order_date#1 as timestamp), MMMM, Some(America/Toronto)) AS order_month#325, count(1) AS total_cnt#326L, first(cast(date_format(cast(order_date#1 as timestamp), MM, Some(America/Toronto)) as int), false) AS month_num#328]\n",
      "   +- Project [order_date#1, customer_id#2L]\n",
      "      +- Relation[order_id#0L,order_date#1,customer_id#2L,order_status#3] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) Sort [month_num#328 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(month_num#328 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#398]\n",
      "   +- *(2) HashAggregate(keys=[customer_id#2L, date_format(cast(order_date#1 as timestamp), MMMM, Some(America/Toronto))#335], functions=[count(1), first(cast(date_format(cast(order_date#1 as timestamp), MM, Some(America/Toronto)) as int), false)], output=[customer_id#2L, order_month#325, total_cnt#326L, month_num#328])\n",
      "      +- Exchange hashpartitioning(customer_id#2L, date_format(cast(order_date#1 as timestamp), MMMM, Some(America/Toronto))#335, 200), ENSURE_REQUIREMENTS, [id=#394]\n",
      "         +- *(1) HashAggregate(keys=[customer_id#2L, date_format(cast(order_date#1 as timestamp), MMMM, Some(America/Toronto)) AS date_format(cast(order_date#1 as timestamp), MMMM, Some(America/Toronto))#335], functions=[partial_count(1), partial_first(cast(date_format(cast(order_date#1 as timestamp), MM, Some(America/Toronto)) as int), false)], output=[customer_id#2L, date_format(cast(order_date#1 as timestamp), MMMM, Some(America/Toronto))#335, count#339L, first#340, valueSet#341])\n",
      "            +- *(1) Project [order_date#1, customer_id#2L]\n",
      "               +- FileScan csv [order_date#1,customer_id#2L,order_status#3] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://m01.itversity.com:9000/public/trendytech/retail_db/ordersnew], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_date:date,customer_id:bigint>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            select customer_id,date_format(order_date,'MMMM') as order_month,\n",
    "            count(*) as total_cnt,first(int(date_format(order_date,'MM'))) as month_num\n",
    "            from order\n",
    "            group by customer_id,order_month\n",
    "            order by month_num      \n",
    "          \"\"\").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6f26d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['month_num ASC NULLS FIRST], true\n",
      "+- 'Aggregate ['customer_id, 'order_month], ['customer_id, 'date_format('order_date, MMMM) AS order_month#342, 'count(1) AS total_cnt#343, first('date_format('order_date, MM), false) AS month_num#345]\n",
      "   +- 'UnresolvedRelation [order], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "customer_id: bigint, order_month: string, total_cnt: bigint, month_num: string\n",
      "Sort [month_num#345 ASC NULLS FIRST], true\n",
      "+- Aggregate [customer_id#2L, date_format(cast(order_date#1 as timestamp), MMMM, Some(America/Toronto))], [customer_id#2L, date_format(cast(order_date#1 as timestamp), MMMM, Some(America/Toronto)) AS order_month#342, count(1) AS total_cnt#343L, first(date_format(cast(order_date#1 as timestamp), MM, Some(America/Toronto)), false) AS month_num#345]\n",
      "   +- SubqueryAlias order\n",
      "      +- Relation[order_id#0L,order_date#1,customer_id#2L,order_status#3] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [month_num#345 ASC NULLS FIRST], true\n",
      "+- Aggregate [customer_id#2L, date_format(cast(order_date#1 as timestamp), MMMM, Some(America/Toronto))], [customer_id#2L, date_format(cast(order_date#1 as timestamp), MMMM, Some(America/Toronto)) AS order_month#342, count(1) AS total_cnt#343L, first(date_format(cast(order_date#1 as timestamp), MM, Some(America/Toronto)), false) AS month_num#345]\n",
      "   +- Project [order_date#1, customer_id#2L]\n",
      "      +- Relation[order_id#0L,order_date#1,customer_id#2L,order_status#3] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) Sort [month_num#345 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(month_num#345 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#437]\n",
      "   +- SortAggregate(key=[customer_id#2L, date_format(cast(order_date#1 as timestamp), MMMM, Some(America/Toronto))#352], functions=[count(1), first(date_format(cast(order_date#1 as timestamp), MM, Some(America/Toronto)), false)], output=[customer_id#2L, order_month#342, total_cnt#343L, month_num#345])\n",
      "      +- *(2) Sort [customer_id#2L ASC NULLS FIRST, date_format(cast(order_date#1 as timestamp), MMMM, Some(America/Toronto))#352 ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(customer_id#2L, date_format(cast(order_date#1 as timestamp), MMMM, Some(America/Toronto))#352, 200), ENSURE_REQUIREMENTS, [id=#432]\n",
      "            +- SortAggregate(key=[customer_id#2L, date_format(cast(order_date#1 as timestamp), MMMM, Some(America/Toronto)) AS date_format(cast(order_date#1 as timestamp), MMMM, Some(America/Toronto))#352], functions=[partial_count(1), partial_first(date_format(cast(order_date#1 as timestamp), MM, Some(America/Toronto)), false)], output=[customer_id#2L, date_format(cast(order_date#1 as timestamp), MMMM, Some(America/Toronto))#352, count#356L, first#357, valueSet#358])\n",
      "               +- *(1) Sort [customer_id#2L ASC NULLS FIRST, date_format(cast(order_date#1 as timestamp), MMMM, Some(America/Toronto)) AS date_format(cast(order_date#1 as timestamp), MMMM, Some(America/Toronto))#352 ASC NULLS FIRST], false, 0\n",
      "                  +- *(1) Project [order_date#1, customer_id#2L]\n",
      "                     +- FileScan csv [order_date#1,customer_id#2L,order_status#3] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://m01.itversity.com:9000/public/trendytech/retail_db/ordersnew], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_date:date,customer_id:bigint>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            select customer_id,date_format(order_date,'MMMM') as order_month,\n",
    "            count(*) as total_cnt,first(date_format(order_date,'MM')) as month_num\n",
    "            from order\n",
    "            group by customer_id,order_month\n",
    "            order by month_num      \n",
    "          \"\"\").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568cd0a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
